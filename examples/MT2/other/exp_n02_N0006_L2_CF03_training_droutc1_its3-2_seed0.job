#!/bin/bash -l
#SBATCH --job-name=MT2
#SBATCH --account="c24"
#SBATCH --mail-type=ALL
#SBATCH --time=24:00:00
#SBATCH --nodes=2
#SBATCH --ntasks-per-core=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --partition=normal
#SBATCH --constraint=gpu
#SBATCH --hint=nomultithreadg
#SBATCH --output=/scratch/snx3000/msalvado/torchbraid3_18_02_2024/torchbraid/examples/machine-translation-2_IWSLT/outputs/c20240828-1824_training/outputs/n02_N0006_L2_CF03_training_droutc1_its3-2_seed0.txt
#SBATCH --error=/scratch/snx3000/msalvado/torchbraid3_18_02_2024/torchbraid/examples/machine-translation-2_IWSLT/outputs/c20240828-1824_training/errors/n02_N0006_L2_CF03_training_droutc1_its3-2_seed0.txt

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export PYTHONUNBUFFERED=true

LOADPATH=/scratch/snx3000/msalvado/torchbraid3_18_02_2024/
RUNPATH=/scratch/snx3000/msalvado/torchbraid3_18_02_2024/torchbraid/examples/machine-translation-2_IWSLT/src/
cd $LOADPATH
source load_modules.sh
cd $RUNPATH

srun python3 -u main.py --batch-size 8 --epochs 1000000 --d_model 512 --dropout .1 --gradient_accumulation 16 --initialize_parameters --num_warmup_steps 8000 --tokenization unigram --vocab_size 8000 --steps 6 --Tf 6. --lp-max-levels 2 --lp-cfactor 3 --lp-fwd-max-iters 3 --lp-bwd-max-iters 2 --seed 0 --num_training_batches 20000
