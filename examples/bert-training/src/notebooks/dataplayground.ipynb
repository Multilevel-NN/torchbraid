{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa3ca97-ebf2-417f-875e-cbae3704435d",
   "metadata": {},
   "source": [
    "We first download all the Wikipedia and bookcorpus datasets. \n",
    "\n",
    "For the wiki set, we only train on text and then concat for the full dataset. We also use a pretrained tokenizer since we're only really interested in the transformer arch. With tokeznier, we convert the text to tokens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ae6d93-4e65-48aa-8ea1-002f0f0c2d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f63afb5-0aaf-491b-ae65-bbf6b262f863",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjiang/hfvenv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "percent_data = .01\n",
    "\n",
    "bookcorpus_train = load_dataset('bookcorpus', split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = wiki_train.remove_columns([col for col in wiki_train.column_names if col != \"text\"]) # Only keep text\n",
    "assert bookcorpus_train.features.type == wiki_train.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus_train, wiki_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b3e1d52-08b1-49a0-9b96-0b3228fe6c21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# 153388\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "def filter_short(examples):\n",
    "    # print(examples)\n",
    "    return len(examples['input_ids']) > 6\n",
    "# print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60e0f85d-d02c-4e8e-814c-61583cc98905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb0349c20e3436f9c340bbc1f49fa75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/742095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 699041\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets))\n",
    "test_set = tokenized_datasets.filter(filter_short)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52e8b635-7801-4881-93ac-bb80fd0665c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2021,\n",
       "  2074,\n",
       "  2028,\n",
       "  2298,\n",
       "  2012,\n",
       "  1037,\n",
       "  7163,\n",
       "  2239,\n",
       "  2741,\n",
       "  2032,\n",
       "  8134,\n",
       "  4937,\n",
       "  22436,\n",
       "  2594,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ece82-4ec2-42cf-be69-d9865edfe6bb",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "\n",
    "We follow the url in the link to pre-process the data. See comments in code but roughly speaking: \n",
    "\n",
    "1. Need to split the data into two \"sentences.\" I do this somewhat arbitrarily and choose a random index within the words.\n",
    "2. Need to replace the tokens and random for the MLM.\n",
    "3. Need to change sentence for the sentence matching for NSP\n",
    "\n",
    "Obtaining the data is actually a bit slow, but that's okay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c72e096-3612-4fbd-93e1-1cfc9f3a3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n",
    "class BERTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Construct a BERT Dataset by \n",
    "\n",
    "    1. Doing next sentence prediction by taking a random integer around half the seq_len\n",
    "    2. Doing the masked language model \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_data: Dataset, tokenizer, seq_len=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Each tokenized data should be a diff sample\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: modify and replace random word with mask / random tokens\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: now put it all together with CLS, SEP and finish with PAD\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4; combine into 1\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "        \n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "    def get_sent(self, index):\n",
    "        \"\"\"\n",
    "        Grab random sentence pair by splitting the tokens randomly. This only samples the first part, but it's okay. \n",
    "\n",
    "        Randomly generates split, then randomly gives correct second or incorrect second sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strip [CLS], [SEP] from each entry and truncate (- 3 so that we need to put back in CLS, and two SEPs)\n",
    "        tokens = self.tokenized_data[index]['input_ids'][1:-1]\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens > self.seq_len - 3:\n",
    "            tokens = tokens[0:self.seq_len - 3]\n",
    "            num_tokens = len(tokens)\n",
    "        \n",
    "        ind_split = random.randrange(1, num_tokens - 1) \n",
    "\n",
    "        # These are the two sentences\n",
    "        t1, t2 = tokens[0:ind_split], tokens[ind_split:]\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            # Need to grab random line and make it correct so that length is less than or equal to original t2\n",
    "            rand_sentence = self.tokenized_data[random.randrange(len(self.tokenized_data))]['input_ids'][1:-1]\n",
    "            if len(t2) >= len(rand_sentence):\n",
    "                t2 = rand_sentence\n",
    "            else:\n",
    "                new_ind = random.randrange(0, len(rand_sentence) - len(t2))\n",
    "                t2 = rand_sentence[new_ind:new_ind + len(t2)]\n",
    "\n",
    "            return t1, t2, 0\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        for i, token_id in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "            \n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token (don't want to give it bad tokens, start from 1000 which is where BERT Tokenizer has good tokens\n",
    "                elif prob < 0.9:\n",
    "                    output.append(random.randrange(1000, len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                output_label.append(0)\n",
    "\n",
    "        # flattening (don't have to do this)\n",
    "        # output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        # output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "\n",
    "        return output, output_label\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f14a590-bd60-4cd3-bf79-f8e78f1e5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BERTDataset(tokenized_datasets, tokenizer, seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0a41fd-fc0a-4dfa-a3c2-3af6251d8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_input': tensor([ 101, 2788, 1010, 2002, 2052,  103,  102,  103,  103, 1996, 2542, 2282,\n",
      "        1010, 2652, 2007, 2010,  103, 1012,  102,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'bert_label': tensor([    0,     0,     0,     0,     0,  2022,     0, 13311,  2105,     0,\n",
      "            0,     0,     0,     0,     0,     0, 10899,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'segment_label': tensor([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'is_next': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    # tokenizer.decode(ds[i]['bert_input'])\n",
    "    print(ds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca80793-e150-4d71-8d31-d616d5bde5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "   ds, batch_size=32, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d351fa3c-9f37-4479-9976-5df29788bfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2417, 20101,  ...,     0,     0,     0],\n",
      "        [  101,  1045, 13673,  ...,     0,     0,     0],\n",
      "        [  101,  2016,   102,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1036,  1036,  ...,     0,     0,     0],\n",
      "        [  101,  1036,   103,  ...,     0,     0,     0],\n",
      "        [  101,  2016,  4711,  ...,     0,     0,     0]]) 0\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(train_loader):\n",
    "    x = x['bert_input']\n",
    "    print(x, i)\n",
    "    # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "785deb71-904e-4dcf-9a4b-db84d891b1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677129db-2c42-4cc1-8918-e726b1b9607f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
