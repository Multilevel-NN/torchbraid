{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b91108d-4820-4c82-9aaa-d5d7c90e281a",
   "metadata": {},
   "source": [
    "# Preprocess and store whole data set so that we get same data each time \n",
    "\n",
    "We will also tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d02bb759-3794-468c-bbb1-69c5c90a10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_dataset import obtain_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb2760b-3462-4171-8785-17eb4cec7025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjiang/hfvenv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for bookcorpus contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bookcorpus\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/sjiang/hfvenv/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/sjiang/hfvenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eb574ff9624f1491ac980e745a2872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74209556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415af99602e4a3586938ad708ffddae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/74209556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds, v_size = obtain_dataset(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcbb09c-352a-4998-b0c7-1027791552f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816 ms ± 17.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26179e1-4bd7-4abd-aff4-a8e96258bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816 ms ± 8.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b868f3-c247-45cc-b62b-b1aabe17b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "ds, batch_size=32, shuffle=True, pin_memory=True, drop_last=True\n",
    ")\n",
    "%timeit next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3014f3d2-a128-438c-8ed4-ec0aa679e3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[  101,  2016,   102,  ...,     0,     0,     0],\n",
       "         [  101,  2023,   102,  ...,     0,     0,     0],\n",
       "         [  101,  2009,  1005,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,   103,   102,  ...,     0,     0,     0],\n",
       "         [  101,   103, 17076,  ...,     0,     0,     0],\n",
       "         [  101,  1996,  2617,  ...,     0,     0,     0]]),\n",
       " 'bert_label': tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   0, 2027,    0,  ...,    0,    0,    0],\n",
       "         [   0, 1998,    0,  ...,    0,    0,    0],\n",
       "         [   0,    0,    0,  ...,    0,    0,    0]]),\n",
       " 'segment_label': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'is_next': tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 0, 1])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e37520e-63ee-471e-948f-1fed6534c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.5 s ± 526 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78ed7d8b-1861-41c6-b5f4-8dff2c616b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2133714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c179d2-7faa-4eeb-af7c-157875fab41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1066857.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2133714 * 30 / 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "54c4ef52-1d74-4eb4-9843-8130a60bd768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Modified from https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n",
    "class MyBERTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Construct a BERT Dataset by \n",
    "\n",
    "    1. Doing next sentence prediction by taking a random integer around half the seq_len\n",
    "    2. Doing the masked language model \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_data: Dataset, tokenizer, seq_len=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Each tokenized data should be a diff sample\n",
    "        \"\"\"\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: modify and replace random word with mask / random tokens\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: now put it all together with CLS, SEP and finish with PAD\n",
    "        # Assuming t1_random and t2_random are already PyTorch tensors\n",
    "        CLS_token = torch.tensor([self.tokenizer.vocab['[CLS]']], dtype=t1_random.dtype, device=t1_random.device)\n",
    "        SEP_token = torch.tensor([self.tokenizer.vocab['[SEP]']], dtype=t1_random.dtype, device=t1_random.device)\n",
    "        PAD_token = torch.tensor([self.tokenizer.vocab['[PAD]']], dtype=t1_label.dtype, device=t1_label.device)\n",
    "\n",
    "        # For t1 and t1_label\n",
    "        t1 = torch.cat((CLS_token, t1_random, SEP_token))  # Insert [CLS] at the beginning and append [SEP] at the end\n",
    "        t1_label = torch.cat((PAD_token, t1_label, PAD_token))  # Insert [PAD] at the beginning and append [PAD] at the end\n",
    "        \n",
    "        # For t2 and t2_label\n",
    "        t2 = torch.cat((t2_random, SEP_token))  # Append [SEP] at the end\n",
    "        t2_label = torch.cat((t2_label, PAD_token))  # Append [PAD] at the end\n",
    "\n",
    "        # Step 4; combine into 1\n",
    "        segment_label_t1 = torch.ones(len(t1), dtype=torch.long, device=t1.device)\n",
    "        segment_label_t2 = torch.full((len(t2),), 2, dtype=torch.long, device=t2.device)  # Using 2 for the second segment\n",
    "        segment_label = torch.cat((segment_label_t1, segment_label_t2))[:self.seq_len]\n",
    "        \n",
    "        # Concatenate t1 and t2 for bert_input and their labels\n",
    "        bert_input = torch.cat((t1, t2))[:self.seq_len]\n",
    "        bert_label = torch.cat((t1_label, t2_label))[:self.seq_len]\n",
    "        \n",
    "        # Padding\n",
    "        PAD_token = self.tokenizer.vocab['[PAD]']\n",
    "        if len(bert_input) < self.seq_len:\n",
    "            padding_length = self.seq_len - len(bert_input)\n",
    "            padding = torch.full((padding_length,), PAD_token, dtype=bert_input.dtype, device=bert_input.device)\n",
    "            bert_input = torch.cat((bert_input, padding))\n",
    "            bert_label = torch.cat((bert_label, padding))\n",
    "            segment_label = torch.cat((segment_label, padding))\n",
    "        \n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_sent(self, index):\n",
    "        \"\"\"\n",
    "        Grab random sentence pair by splitting the tokens randomly. This only samples the first part, but it's okay. \n",
    "\n",
    "        Randomly generates split, then randomly gives correct second or incorrect second sentence\n",
    "        \"\"\"\n",
    "        # Strip [CLS], [SEP] from each entry and truncate (- 3 so that we need to put back in CLS, and two SEPs)\n",
    "        tokens = self.tokenized_data[index]['input_ids'][1:-1]\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens > self.seq_len - 3:\n",
    "            tokens = tokens[0:self.seq_len - 3]\n",
    "            num_tokens = len(tokens)\n",
    "\n",
    "        ind_split = random.randrange(1, num_tokens - 1) \n",
    "\n",
    "        # These are the two sentences\n",
    "        t1, t2 = tokens[0:ind_split], tokens[ind_split:]\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return torch.tensor(t1), torch.tensor(t2), 1\n",
    "        else:\n",
    "            # Need to grab random line and make it correct so that length is less than or equal to original t2\n",
    "            rand_sentence = self.tokenized_data[random.randrange(len(self.tokenized_data))]['input_ids'][1:-1]\n",
    "            if len(t2) >= len(rand_sentence):\n",
    "                t2 = rand_sentence\n",
    "            else:\n",
    "                new_ind = random.randrange(0, len(rand_sentence) - len(t2))\n",
    "                t2 = rand_sentence[new_ind:new_ind + len(t2)]\n",
    "\n",
    "            return torch.tensor(t1), torch.tensor(t2), 0\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        # Assuming 'sentence' is a PyTorch tensor\n",
    "        output_label = torch.zeros_like(sentence)\n",
    "        output = sentence.clone()  # Create a copy of the input tensor for output\n",
    "    \n",
    "        # Calculate probabilities for each token in one go\n",
    "        probs = torch.rand(sentence.size())\n",
    "        mask_indices = (probs < 0.15).nonzero(as_tuple=True)[0]  # Indices where tokens will be modified\n",
    "    \n",
    "        # Calculate sub-probabilities for actions within the 15% chance\n",
    "        action_probs = torch.rand(mask_indices.size(0))\n",
    "    \n",
    "        # 80% chance change token to mask token\n",
    "        mask_tokens = mask_indices[action_probs < 0.8]\n",
    "        output[mask_tokens] = self.tokenizer.vocab['[MASK]']\n",
    "    \n",
    "        # 10% chance change token to random token\n",
    "        random_tokens = mask_indices[(action_probs >= 0.8) & (action_probs < 0.9)]\n",
    "        if len(random_tokens) > 0:\n",
    "            output[random_tokens] = torch.randint(1000, len(self.tokenizer.vocab), (len(random_tokens),))\n",
    "    \n",
    "        # For the 10% chance to keep the same token, no action is needed as we've copied the original tokens\n",
    "    \n",
    "        # Update output_label for changed tokens\n",
    "        output_label[mask_indices] = sentence[mask_indices]\n",
    "    \n",
    "        return output, output_label        \n",
    "\n",
    "def obtain_dataset(percent_data:float = 0.01, seq_len: int = 128):\n",
    "    \"\"\"\n",
    "    See Jupyter for logic here\n",
    "    \"\"\"\n",
    "    # Hard code for now\n",
    "    bookcorpus_train = load_dataset('bookcorpus', split=f'train[:{int(percent_data * 100)}%]')\n",
    "    wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[:{int(percent_data * 100)}%]')\n",
    "\n",
    "    # bookcorpus_train = load_dataset('bookcorpus', split=f'train[0:25000]')\n",
    "    # wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[0:25000]')\n",
    "\n",
    "    # bookcorpus_train = load_dataset('bookcorpus', split=f'train[0:100]')\n",
    "    # wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[0:100]')\n",
    "\n",
    "    wiki_train = wiki_train.remove_columns([col for col in wiki_train.column_names if col != \"text\"]) # Only keep text\n",
    "    assert bookcorpus_train.features.type == wiki_train.features.type\n",
    "    raw_datasets = concatenate_datasets([bookcorpus_train, wiki_train])\n",
    "\n",
    "    # Load pretrained \n",
    "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def group_texts(examples):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "        )\n",
    "        \n",
    "        return tokenized_inputs\n",
    "    \n",
    "    def filter_short(examples):\n",
    "        return len(examples['input_ids']) > 6\n",
    "\n",
    "    # preprocess dataset\n",
    "    tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"], num_proc=8).filter(\n",
    "        filter_short, num_proc=8\n",
    "    )\n",
    "    # print(tokenized_datasets)\n",
    "\n",
    "    return MyBERTDataset(tokenized_datasets, tokenizer, seq_len), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "28767d86-09ff-47db-b410-bb17beec5e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f143bc26216841e5a94185ad67931e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/7420956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f793cbef7284c33a93ac9e0220eda03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/7420956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds, _ = obtain_dataset(.1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a20089ee-892d-423a-93cc-3fed1c86caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.4 ms ± 872 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9fba349b-a2af-48ae-a68c-39c8d40ece84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "ds, batch_size=64, shuffle=True, pin_memory=True, drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7534d274-2f8f-48f7-8a98-805cc87f5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.59 s ± 24.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "03bd98b5-ab44-4860-9e88-443971b19723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1535.0886833333332"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2133714 *2.59 / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a2902cc9-1ef6-49d8-a846-b854f0cd4b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21845"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c741cef5-8e34-425e-9fb1-b69b8c510135",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.rand(10)\n",
    "mask_indices = (probs < 0.15).nonzero(as_tuple=True)[0]  # Indices where tokens will be modified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f806de3c-e149-40b0-9f82-6ffa39930cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]),)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(probs < 0.15).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1480ba-568f-4348-842f-02a6d7a2f7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
