{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2694eb16-68ac-49e7-ae01-095c3026f581",
   "metadata": {},
   "source": [
    "# Fine tuning, for the n=2 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e6a5ce-3981-4d39-b705-b63ebfb4fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Needed for parallel \n",
    "from collections import OrderedDict\n",
    "\n",
    "# For training \n",
    "from network_architecture_v2 import MyBertForSequenceClassification\n",
    "\n",
    "# For fine tuning\n",
    "from datasets import load_dataset #, load_metric\n",
    "from transformers import BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae89cba-2784-4665-9919-f0a8cab23a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjiang/braids/pip-test/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53002643a38a48db9b0c1b6f91eae266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0b9940ba8240e586cb9272b3172350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbf5b72d31e4660a2f5ae2a4c8d59ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "# I believe this is the tokenizer I used... \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", \n",
    "                     max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec77fb-e953-4d92-b916-06582905c5e4",
   "metadata": {},
   "source": [
    "# Load the parallel model\n",
    "\n",
    "This involves a bit more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ef052a-d8e9-47d9-a553-f5dbdbb59e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_0 = torch.load('bert-save-2/model_checkpoint_0_batch_idx=80000')\n",
    "checkpoint_1 = torch.load('bert-save-2/model_checkpoint_1_batch_idx=80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4888000a-c1bb-464e-bd09-765783512755",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_0 = checkpoint_0['model_state'].keys()\n",
    "keys_1 = checkpoint_1['model_state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11a8cd2-eed6-4d36-a144-8f785067873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Ugh, this is \n",
    "new_dict = OrderedDict()\n",
    "keys_0 = checkpoint_0['model_state'].keys()\n",
    "counter = 0\n",
    "for key in keys_0:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        if int(split[2]) > counter:\n",
    "            counter = int(split[2])\n",
    "            \n",
    "        split.insert(3, 'layer')\n",
    "        new_key = '.'.join(split[1:])\n",
    "        new_dict[new_key] = checkpoint_0['model_state'][key]\n",
    "    else:\n",
    "        new_key = key\n",
    "        if 'close_nsp' in key:\n",
    "            # print(key)\n",
    "            split = key.split('.')\n",
    "            split[0] = 'close_nn_nsp'\n",
    "            new_key = '.'.join(split)\n",
    "        if 'close_mlm' in key:\n",
    "            # print(key)\n",
    "            split = key.split('.')\n",
    "            split[0] = 'close_nn_mlm'\n",
    "            new_key = '.'.join(split)\n",
    "        \n",
    "        new_dict[new_key] = checkpoint_0['model_state'][key]\n",
    "print(counter)\n",
    "\n",
    "# Now for the remaining parts? \n",
    "keys_1 = checkpoint_1['model_state'].keys()\n",
    "for key in keys_1:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        \n",
    "        new_key = '.'.join(split[1:])\n",
    "        new_dict[new_key] = checkpoint_1['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_1['model_state'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c3dbbc-92eb-4897-b9ba-0983be1db15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parallel = torch.load('serialnet_bert_32')\n",
    "model_parallel.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd88bd1-6029-4a35-b41e-d4893f26af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parallel = MyBertForSequenceClassification(model_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afbfa0-6695-4278-879d-051d6ace0293",
   "metadata": {},
   "source": [
    "# Now let's train\n",
    "\n",
    "https://proceedings.neurips.cc/paper_files/paper/2023/file/095a6917768712b7ccc61acbeecad1d8-Supplemental-Conference.pdf for hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c0cba5-6640-4aa6-bf37-88524e7890dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-6,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e7efdd-fdaf-4a1b-b443-832b35e1b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(np.float32).mean().item()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ce3905-cbeb-46b0-a217-d2403ac045d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_parallel,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c177c5b-3235-4aac-8663-e98167d8d55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 1:47:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.473321</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.490394</td>\n",
       "      <td>0.831019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.834491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6312, training_loss=0.2275288417057166, metrics={'train_runtime': 6447.1197, 'train_samples_per_second': 31.339, 'train_steps_per_second': 0.979, 'total_flos': 0.0, 'train_loss': 0.2275288417057166, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f517e8-792b-48fe-846b-7c6290ba96ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251241218"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in training_parallel.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2c59eb2-5633-43eb-9764-e9586cec8b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6312' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6312/6312 1:48:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.482051</td>\n",
       "      <td>0.822917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.536374</td>\n",
       "      <td>0.826389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.645903</td>\n",
       "      <td>0.839120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6312, training_loss=0.08186612888392773, metrics={'train_runtime': 6501.9192, 'train_samples_per_second': 31.075, 'train_steps_per_second': 0.971, 'total_flos': 0.0, 'train_loss': 0.08186612888392773, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_parallel = MyBertForSequenceClassification(model_parallel)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf6148a-3c53-4284-a75f-4940d1483074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjiang/braids/pip-test/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'cola')\n",
    "\n",
    "# I believe this is the tokenizer I used... \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", \n",
    "                     max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db36963-7587-474c-93d0-f794383a3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parallel = MyBertForSequenceClassification(model_parallel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3dee28-986e-4f27-8c65-c0b5db066f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-6,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_parallel,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "003ec44b-eaa3-4cb2-9b1a-4ba55684db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [801/801 10:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.625900</td>\n",
       "      <td>0.688477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>0.616845</td>\n",
       "      <td>0.683594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.719184</td>\n",
       "      <td>0.676758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=801, training_loss=0.5481547378794234, metrics={'train_runtime': 613.5173, 'train_samples_per_second': 41.813, 'train_steps_per_second': 1.306, 'total_flos': 0.0, 'train_loss': 0.5481547378794234, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For COLA\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227405d0-f165-4e69-9efe-7daff156f06f",
   "metadata": {},
   "source": [
    "# MRPC \n",
    "\n",
    "Reload models, then retrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f14b1a-fa0e-489d-842c-72e4e6928feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parallel = torch.load('serialnet_bert_32')\n",
    "model_parallel.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c9fd607-0afb-461f-980b-328fbf4b92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parallel = MyBertForSequenceClassification(model_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d54eedcf-b860-4e38-a49c-fe58915c46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'mrpc')\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"], \n",
    "        examples[\"sentence2\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b860226a-0688-4d8f-8646-3087e685e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-8,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_parallel,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49b6981e-15d7-4064-b788-3c40a93c44a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1145' max='1145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1145/1145 19:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.586236</td>\n",
       "      <td>0.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605300</td>\n",
       "      <td>0.578408</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>0.647816</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.716277</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>0.817811</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1145, training_loss=0.44530555676164585, metrics={'train_runtime': 1163.2257, 'train_samples_per_second': 15.767, 'train_steps_per_second': 0.984, 'total_flos': 0.0, 'train_loss': 0.44530555676164585, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For MRPC\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112479e3-123f-4015-9349-b7373003adf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
