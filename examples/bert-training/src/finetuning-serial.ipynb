{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cbb2658-b6c2-4ef2-a909-35a8465b3829",
   "metadata": {},
   "source": [
    "# Benchmark serial case\n",
    "\n",
    "We first load the data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "770c9c19-b63d-4e45-b57b-799be721ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Needed for parallel \n",
    "from collections import OrderedDict\n",
    "\n",
    "# For training \n",
    "from network_architecture_v2 import MyBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5b47a61-f632-4e15-af43-f77fba6cafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tuning\n",
    "from datasets import load_dataset #, load_metric\n",
    "from transformers import BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa5cdc28-4739-444a-8630-5055057b78c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjiang/braids/pip-test/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "# I believe this is the tokenizer I used... \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", \n",
    "                     max_length=64, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cba4d-c563-4a39-bacb-d07036b02755",
   "metadata": {},
   "source": [
    "# Load the saved model! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a917b4c-d1f0-46d3-a195-b17e85433855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "model_dicts = torch.load(f'bert-save-1/model_serial_checkpoint_batch_idx=80000')\n",
    "new_dict = OrderedDict(model_dicts['model_state_dict'])\n",
    "# Load actual model \n",
    "model_serial = torch.load('serialnet_bert_32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94a87e-6159-44c8-be06-98dde36f8a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68430679-1573-41ec-8f7e-23a4a5534f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_serial.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d7583-6dbf-4faa-9a40-43772a2544ff",
   "metadata": {},
   "source": [
    "# With model loaded, time to change the final layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e1b4ddb-0cf3-49ab-a7b3-21b73639d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_serial = MyBertForSequenceClassification(model_serial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e36d26-6254-4fce-90d4-a0d3dc8e2faa",
   "metadata": {},
   "source": [
    "# Define trainers and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dfb4ffa-bdfd-4a88-9d9c-5a98d7570aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-6,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee53ffb6-e8af-499f-b780-f7d6300f7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).astype(np.float32).mean().item()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a684d80f-c0ff-4921-ac33-c1dcc97d91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_serial,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35fb574f-6107-40e5-a5d8-bf0349bd0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251241218"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in training_serial.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14491e70-ef47-4e09-a1e4-d9bb3e2f9e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/6312 08:20 < 16:23, 4.25 it/s, Epoch 1.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251800</td>\n",
       "      <td>0.390535</td>\n",
       "      <td>0.826389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/braids/pip-test/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/braids/pip-test/lib/python3.10/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b788e7-037c-4de1-a0f4-e22ae7efc2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2869' max='6312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2869/6312 20:36 < 24:44, 2.32 it/s, Epoch 1.36/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.468917</td>\n",
       "      <td>0.840278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_serial = MyBertForSequenceClassification(model_serial)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f29318-15f8-4c82-bf84-664ef8c54aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'cola')\n",
    "\n",
    "# I believe this is the tokenizer I used... \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", \n",
    "                     max_length=128, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7594e90-c28e-425d-a444-00beea4d60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_serial = MyBertForSequenceClassification(model_serial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd41b8-20bf-41b8-9977-c836af3fed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-6,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_serial,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f70d4e-1f25-4b86-827e-bbc939f74889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For COLA\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38864e88-5149-45ac-bb51-9965044fa45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'mrpc')\n",
    "training_serial = MyBertForSequenceClassification(model_serial)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"], \n",
    "        examples[\"sentence2\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08d038a-1b6c-43c5-b5b7-db07f64aa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4281b3-f237-4ff8-9830-d06d70e6c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "model_dicts = torch.load(f'bert-save-1/model_serial_checkpoint_batch_idx=80000')\n",
    "new_dict = OrderedDict(model_dicts['model_state'])\n",
    "# Load actual model \n",
    "model_serial = torch.load('serialnet_bert_32')\n",
    "\n",
    "model_serial.load_state_dict(new_dict)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.988,\n",
    "    adam_epsilon=1e-8,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=5,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_serial,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e201cc-b1a3-495a-9f75-684cd836724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MRPC\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66ce6d-b9d6-45c5-b555-37ee712cad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "optimizer = trainer.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86700e0e-e6f0-43b0-84ea-28691b4c7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_schedule = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=10,  # Number of warmup steps\n",
    "    num_training_steps=800000   # Total number of training steps\n",
    ")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "learning_rates = []\n",
    "for step in range(800000):\n",
    "    # Get the learning rate for the current step\n",
    "    lr = optim_schedule.get_lr()[0]  # Get the learning rate for the first parameter group\n",
    "    learning_rates.append(lr)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    optim_schedule.step()\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(learning_rates)\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbbb73-f3c5-4e40-af78-465a011712d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_schedule\n",
    "optim_schedule.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b2d4e-b34c-4ef9-9530-02026d98bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_schedule.step()\n",
    "optim_schedule.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db25122-8b33-4975-9d1f-d48862520985",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cb6d8-e762-40b5-8e1a-6418baf6c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
