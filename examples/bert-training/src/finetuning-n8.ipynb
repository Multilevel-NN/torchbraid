{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfec7a5-71cc-4b9c-8bb6-2cf8401d0620",
   "metadata": {},
   "source": [
    "# Fine tuning, for the n=8 case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b3e257-4175-4435-bb4a-973687f3b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Needed for parallel \n",
    "from collections import OrderedDict\n",
    "\n",
    "# For training \n",
    "from network_architecture_v2 import MyBertForSequenceClassification\n",
    "\n",
    "# For fine tuning\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ce68e2-8aa9-45fb-b3f5-4e6299af933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "# I believe this is the tokenizer I used... \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", \n",
    "                     max_length=64, truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce7f01-a264-4011-b53c-73711827c42a",
   "metadata": {},
   "source": [
    "# Load the parallel model\n",
    "\n",
    "This involves a bit more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c63e211-59bd-4232-b886-72c19ea9cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_0 = torch.load('models_32/model_checkpoint_procs=8_0_epoch=1')\n",
    "checkpoint_1 = torch.load('models_32/model_checkpoint_procs=8_1_epoch=1')\n",
    "checkpoint_2 = torch.load('models_32/model_checkpoint_procs=8_2_epoch=1')\n",
    "checkpoint_3 = torch.load('models_32/model_checkpoint_procs=8_3_epoch=1')\n",
    "checkpoint_4 = torch.load('models_32/model_checkpoint_procs=8_4_epoch=1')\n",
    "checkpoint_5 = torch.load('models_32/model_checkpoint_procs=8_5_epoch=1')\n",
    "checkpoint_6 = torch.load('models_32/model_checkpoint_procs=8_6_epoch=1')\n",
    "checkpoint_7 = torch.load('models_32/model_checkpoint_procs=8_7_epoch=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcedd770-39cb-48bb-b246-65044ef7f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_0 = checkpoint_0['model_state'].keys()\n",
    "keys_1 = checkpoint_1['model_state'].keys()\n",
    "keys_2 = checkpoint_2['model_state'].keys()\n",
    "keys_3 = checkpoint_3['model_state'].keys()\n",
    "keys_4 = checkpoint_4['model_state'].keys()\n",
    "keys_5 = checkpoint_5['model_state'].keys()\n",
    "keys_6 = checkpoint_6['model_state'].keys()\n",
    "keys_7 = checkpoint_7['model_state'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8dfda31-b6cc-47c3-beca-3337a6020513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "# Ugh, this is so dumb\n",
    "new_dict = OrderedDict()\n",
    "keys_0 = checkpoint_0['model_state'].keys()\n",
    "counter = 0\n",
    "for key in keys_0:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        if int(split[2]) > counter:\n",
    "            counter = int(split[2])\n",
    "            \n",
    "        split.insert(3, 'layer')\n",
    "        new_key = '.'.join(split[1:])\n",
    "        new_dict[new_key] = checkpoint_0['model_state'][key]\n",
    "    else:\n",
    "        new_key = key\n",
    "        if 'close_nsp' in key:\n",
    "            # print(key)\n",
    "            split = key.split('.')\n",
    "            split[0] = 'close_nn_nsp'\n",
    "            new_key = '.'.join(split)\n",
    "        if 'close_mlm' in key:\n",
    "            # print(key)\n",
    "            split = key.split('.')\n",
    "            split[0] = 'close_nn_mlm'\n",
    "            new_key = '.'.join(split)\n",
    "        \n",
    "        new_dict[new_key] = checkpoint_0['model_state'][key]\n",
    "        \n",
    "print(counter)\n",
    "\n",
    "# Now for the remaining parts? \n",
    "keys_1 = checkpoint_1['model_state'].keys()\n",
    "new_counter = 0\n",
    "for key in keys_1:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_1['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_1['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_2 = checkpoint_2['model_state'].keys()\n",
    "for key in keys_2:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_2['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_2['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_3 = checkpoint_3['model_state'].keys()\n",
    "for key in keys_3:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_3['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_3['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_4 = checkpoint_4['model_state'].keys()\n",
    "for key in keys_4:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_4['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_4['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_5 = checkpoint_5['model_state'].keys()\n",
    "for key in keys_5:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_5['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_5['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_6 = checkpoint_6['model_state'].keys()\n",
    "for key in keys_6:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_6['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_6['model_state'][key]\n",
    "\n",
    "print(new_counter)\n",
    "counter = new_counter\n",
    "new_counter = 0\n",
    "keys_7 = checkpoint_7['model_state'].keys()\n",
    "for key in keys_7:\n",
    "    if 'parallel_nn' in key:\n",
    "        split = key.split('.')\n",
    "        split[1] = 'serial_nn'\n",
    "        split[2] = str(int(split[2]) + counter + 1)\n",
    "        split.insert(3, 'layer')\n",
    "        if int(split[2]) > new_counter:\n",
    "            new_counter = int(split[2])\n",
    "        new_key = '.'.join(split[1:])\n",
    "        # print(key, new_key)\n",
    "        new_dict[new_key] = checkpoint_7['model_state'][key]\n",
    "    else:\n",
    "        new_dict[key] = checkpoint_7['model_state'][key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd0037f-741a-4701-a298-4f43c24b4583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parallel = torch.load('models_32/serialnet_bert_32')\n",
    "model_parallel.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134dd38c-de65-43e8-b12f-85f6276f4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parallel = MyBertForSequenceClassification(model_parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d14e1-b134-4dd2-82b6-c9ca533198b7",
   "metadata": {},
   "source": [
    "# With weights loaded, go ahead and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffab2a4-c402-462f-9c6d-34ae71963e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    "    learning_rate=1e-4,\n",
    "    # adam_beta1=0.9,\n",
    "    # adam_beta2=0.988,\n",
    "    # adam_epsilon=1e-6,\n",
    "    dataloader_drop_last=True,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa6144e-a7d0-4ba0-ab5d-b7e468eb9134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1904451/460567621.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "# Load the accuracy metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# Define the compute metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1c8efc-4059-43b4-814c-66e424ccc13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=training_parallel,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bed8ba0-72cf-4c47-93a7-c5e514966487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22448' max='22448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22448/22448 1:53:13, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540200</td>\n",
       "      <td>0.538670</td>\n",
       "      <td>0.732639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.466151</td>\n",
       "      <td>0.797454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368000</td>\n",
       "      <td>0.483167</td>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.482205</td>\n",
       "      <td>0.798611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.802083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.600537</td>\n",
       "      <td>0.797454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.619955</td>\n",
       "      <td>0.790509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.649176</td>\n",
       "      <td>0.792824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22448, training_loss=0.32637552623334987, metrics={'train_runtime': 6793.7025, 'train_samples_per_second': 79.308, 'train_steps_per_second': 3.304, 'total_flos': 0.0, 'train_loss': 0.32637552623334987, 'epoch': 8.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f272254a-3013-4b00-a24a-df9cbf42930f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435913730"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in training_parallel.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a7b0b-8577-4a34-ad68-64210c144eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
