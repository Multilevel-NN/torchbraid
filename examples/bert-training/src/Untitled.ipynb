{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db2ff71-9cca-46e5-afc9-af82748b2e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (4.40.2)\n",
      "Requirement already satisfied: filelock in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /global/u1/s/sjiang12/.local/perlmutter/pytorch2.0.1/lib/python3.9/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: filelock in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (3.12.0)\n",
      "Requirement already satisfied: pandas in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Collecting pyarrow>=12.0.0\n",
      "  Downloading pyarrow-16.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: packaging in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.8/193.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, dill, multiprocess, datasets\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26117f3e-fbc4-4ae5-8d82-54a816dda0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, concatenate_datasets\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizerFast\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m percent_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.01\u001b[39m\n\u001b[1;32m      7\u001b[0m bookcorpus_train \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbookcorpus\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(percent_data\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "percent_data = .01\n",
    "\n",
    "bookcorpus_train = load_dataset('bookcorpus', split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = wiki_train.remove_columns([col for col in wiki_train.column_names if col != \"text\"]) # Only keep text\n",
    "assert bookcorpus_train.features.type == wiki_train.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus_train, wiki_train])\n",
    "\n",
    "# Load pretrained \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"])\n",
    "# print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c3577ea-e2db-4a9f-89ce-b45502283c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello. what are you. i am god [SEP]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('Hello. What are you. I am god')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4094d36-378a-45b9-8d05-4d714d1e60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2788,\n",
       " 1010,\n",
       " 2002,\n",
       " 2052,\n",
       " 2022,\n",
       " 13311,\n",
       " 2105,\n",
       " 1996,\n",
       " 2542,\n",
       " 2282,\n",
       " 1010,\n",
       " 2652,\n",
       " 2007,\n",
       " 2010,\n",
       " 10899,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c72e096-3612-4fbd-93e1-1cfc9f3a3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "class BERTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Construct a BERT Dataset by \n",
    "\n",
    "    1. Doing next sentence prediction by taking a random integer around half the seq_len\n",
    "    2. Doing the masked language model \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_data: Dataset, tokenizer, seq_len=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "        \n",
    "    def get_sent(self, index):\n",
    "        \"\"\"\n",
    "        Grab random sentence pair by splitting the tokens randomly. This only samples the first part, but it's okay. \n",
    "\n",
    "        Randomly generates split, then randomly gives correct second or incorrect second sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strip [CLS], [SEP] from each entry and truncate (- 3 so that we need to put back in CLS, and two SEPs)\n",
    "        tokens = self.tokenized_data[index]['input_ids'][1:-1]\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens > self.seq_len - 3:\n",
    "            tokens = tokens[0:self.seq_len - 3]\n",
    "            num_tokens = len(tokens)\n",
    "        \n",
    "        ind_split = random.randrange(1, num_tokens - 1) \n",
    "\n",
    "        # These are the two sentences\n",
    "        t1, t2 = tokens[0:ind_split], tokens[ind_split:]\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            # Need to grab random line and make it correct so that length is less than or equal to original t2\n",
    "            rand_sentence = self.tokenized_data[random.randrange(len(self.tokenized_data))]['input_ids'][1:-1]\n",
    "            if len(t2) >= len(rand_sentence):\n",
    "                t2 = rand_sentence\n",
    "            else:\n",
    "                new_ind = random.randrange(0, len(rand_sentence) - len(t2))\n",
    "                t2 = rand_sentence[new_ind:new_ind + len(t2)]\n",
    "\n",
    "            return t1, t2, 0\n",
    "            \n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        for i, token_id in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "            \n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token (don't want to give it bad tokens, start from 1000 which is where BERT Tokenizer has good tokens\n",
    "                elif prob < 0.9:\n",
    "                    output.append(random.randrange(1000, len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "\n",
    "        return output, output_label\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: modify and replace random word with mask / random tokens\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: now put it all together with CLS, SEP and finish with PAD\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4; combine into 1\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "        \n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3f14a590-bd60-4cd3-bf79-f8e78f1e5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BERTDataset(tokenized_datasets, tokenizer, seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e0a41fd-fc0a-4dfa-a3c2-3af6251d8c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] a japanese [MASK]ku poet and painter, [MASK] referred to simply as \" bus [MASK] \". [MASK] is one of the greatest poets of the edo [MASK], comparable with matsuo basho and kobayashi issa. he was born in osaka, his real family name was [SEP] taniguchi [MASK] ). from 1758, he began [MASK] live in kyoto, and he stayed there for restmasters his life. he got married when he was 45 years old, and became a father of his daughter [MASK] [MASK] ( ). references 1716 births 1784 deaths japanese artists japanese poets people from [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds[-200]['bert_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9ca80793-e150-4d71-8d31-d616d5bde5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "   ds, batch_size=32, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d351fa3c-9f37-4479-9976-5df29788bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(train_loader):\n",
    "    # x = x['bert_input']\n",
    "    # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e451181f-f547-47cd-b7f8-cc7104f2c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (x['bert_input'] > 0).unsqueeze(1).repeat(1, x['bert_input'].size(1), 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5bf7e99-9ea2-48ad-b7a1-d91f159ba7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "14671e5a-bfa6-429e-ad24-ef2df5454689",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadAttention(20, 4, batch_first=True, dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "710c989e-2ee1-4b1f-aec5-bc3f27820c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.randint(low=0, high=2, size=(8, 8))\n",
    "mask = mask > 0\n",
    "x = torch.rand((4, 8, 20))\n",
    "attn_output, _ = mha(x, x, x, attn_mask=mask, need_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d6c7f029-6ce2-41b6-9104-b2043fc68a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 128])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677129db-2c42-4cc1-8918-e726b1b9607f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
