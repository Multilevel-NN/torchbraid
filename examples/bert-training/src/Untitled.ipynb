{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26117f3e-fbc4-4ae5-8d82-54a816dda0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "percent_data = .01\n",
    "\n",
    "bookcorpus_train = load_dataset('bookcorpus', split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = load_dataset(\"wikipedia\", \"20220301.simple\", split=f'train[:{int(percent_data * 100)}%]')\n",
    "wiki_train = wiki_train.remove_columns([col for col in wiki_train.column_names if col != \"text\"]) # Only keep text\n",
    "assert bookcorpus_train.features.type == wiki_train.features.type\n",
    "raw_datasets = concatenate_datasets([bookcorpus_train, wiki_train])\n",
    "\n",
    "# Load pretrained \n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_datasets = raw_datasets.map(group_texts, batched=True, remove_columns=[\"text\"])\n",
    "# print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c3577ea-e2db-4a9f-89ce-b45502283c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello. what are you. i am god [SEP]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer('Hello. What are you. I am god')['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4094d36-378a-45b9-8d05-4d714d1e60cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2788,\n",
       " 1010,\n",
       " 2002,\n",
       " 2052,\n",
       " 2022,\n",
       " 13311,\n",
       " 2105,\n",
       " 1996,\n",
       " 2542,\n",
       " 2282,\n",
       " 1010,\n",
       " 2652,\n",
       " 2007,\n",
       " 2010,\n",
       " 10899,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5c72e096-3612-4fbd-93e1-1cfc9f3a3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://medium.com/data-and-beyond/complete-guide-to-building-bert-model-from-sratch-3e6562228891\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import itertools\n",
    "class BERTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Construct a BERT Dataset by \n",
    "\n",
    "    1. Doing next sentence prediction by taking a random integer around half the seq_len\n",
    "    2. Doing the masked language model \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenized_data: Dataset, tokenizer, seq_len=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "        \n",
    "    def get_sent(self, index):\n",
    "        \"\"\"\n",
    "        Grab random sentence pair by splitting the tokens randomly. This only samples the first part, but it's okay. \n",
    "\n",
    "        Randomly generates split, then randomly gives correct second or incorrect second sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        # Strip [CLS], [SEP] from each entry and truncate (- 3 so that we need to put back in CLS, and two SEPs)\n",
    "        tokens = self.tokenized_data[index]['input_ids'][1:-1]\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens > self.seq_len - 3:\n",
    "            tokens = tokens[0:self.seq_len - 3]\n",
    "            num_tokens = len(tokens)\n",
    "        \n",
    "        ind_split = random.randrange(1, num_tokens - 1) \n",
    "\n",
    "        # These are the two sentences\n",
    "        t1, t2 = tokens[0:ind_split], tokens[ind_split:]\n",
    "\n",
    "        # negative or positive pair, for next sentence prediction\n",
    "        if random.random() > 0.5:\n",
    "            return t1, t2, 1\n",
    "        else:\n",
    "            # Need to grab random line and make it correct so that length is less than or equal to original t2\n",
    "            rand_sentence = self.tokenized_data[random.randrange(len(self.tokenized_data))]['input_ids'][1:-1]\n",
    "            if len(t2) >= len(rand_sentence):\n",
    "                t2 = rand_sentence\n",
    "            else:\n",
    "                new_ind = random.randrange(0, len(rand_sentence) - len(t2))\n",
    "                t2 = rand_sentence[new_ind:new_ind + len(t2)]\n",
    "\n",
    "            return t1, t2, 0\n",
    "            \n",
    "    def random_word(self, sentence):\n",
    "        output_label = []\n",
    "        output = []\n",
    "\n",
    "        for i, token_id in enumerate(sentence):\n",
    "            prob = random.random()\n",
    "            \n",
    "            # 15% of the tokens would be replaced\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% chance change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    output.append(self.tokenizer.vocab['[MASK]'])\n",
    "\n",
    "                # 10% chance change token to random token (don't want to give it bad tokens, start from 1000 which is where BERT Tokenizer has good tokens\n",
    "                elif prob < 0.9:\n",
    "                    output.append(random.randrange(1000, len(self.tokenizer.vocab)))\n",
    "\n",
    "                # 10% chance change token to current token\n",
    "                else:\n",
    "                    output.append(token_id)\n",
    "\n",
    "                output_label.append(token_id)\n",
    "\n",
    "            else:\n",
    "                output.append(token_id)\n",
    "                output_label.append(0)\n",
    "\n",
    "        # flattening\n",
    "        output = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output]))\n",
    "        output_label = list(itertools.chain(*[[x] if not isinstance(x, list) else x for x in output_label]))\n",
    "        assert len(output) == len(output_label)\n",
    "\n",
    "        return output, output_label\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        # Step 1: get random sentence pair, either negative or positive (saved as is_next_label)\n",
    "        t1, t2, is_next_label = self.get_sent(item)\n",
    "\n",
    "        # Step 2: modify and replace random word with mask / random tokens\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "        t2_random, t2_label = self.random_word(t2)\n",
    "\n",
    "        # Step 3: now put it all together with CLS, SEP and finish with PAD\n",
    "        t1 = [self.tokenizer.vocab['[CLS]']] + t1_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t2 = t2_random + [self.tokenizer.vocab['[SEP]']]\n",
    "        t1_label = [self.tokenizer.vocab['[PAD]']] + t1_label + [self.tokenizer.vocab['[PAD]']]\n",
    "        t2_label = t2_label + [self.tokenizer.vocab['[PAD]']]\n",
    "\n",
    "        # Step 4; combine into 1\n",
    "        segment_label = ([1 for _ in range(len(t1))] + [2 for _ in range(len(t2))])[:self.seq_len]\n",
    "        bert_input = (t1 + t2)[:self.seq_len]\n",
    "        bert_label = (t1_label + t2_label)[:self.seq_len]\n",
    "        padding = [self.tokenizer.vocab['[PAD]'] for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "        \n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  \"is_next\": is_next_label}\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3f14a590-bd60-4cd3-bf79-f8e78f1e5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = BERTDataset(tokenized_datasets, tokenizer, seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e0a41fd-fc0a-4dfa-a3c2-3af6251d8c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] a japanese [MASK]ku poet and painter, [MASK] referred to simply as \" bus [MASK] \". [MASK] is one of the greatest poets of the edo [MASK], comparable with matsuo basho and kobayashi issa. he was born in osaka, his real family name was [SEP] taniguchi [MASK] ). from 1758, he began [MASK] live in kyoto, and he stayed there for restmasters his life. he got married when he was 45 years old, and became a father of his daughter [MASK] [MASK] ( ). references 1716 births 1784 deaths japanese artists japanese poets people from [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ds[-200]['bert_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9ca80793-e150-4d71-8d31-d616d5bde5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(\n",
    "   ds, batch_size=32, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d351fa3c-9f37-4479-9976-5df29788bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(train_loader):\n",
    "    # x = x['bert_input']\n",
    "    # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e451181f-f547-47cd-b7f8-cc7104f2c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (x['bert_input'] > 0).unsqueeze(1).repeat(1, x['bert_input'].size(1), 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5bf7e99-9ea2-48ad-b7a1-d91f159ba7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "14671e5a-bfa6-429e-ad24-ef2df5454689",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadAttention(20, 4, batch_first=True, dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "710c989e-2ee1-4b1f-aec5-bc3f27820c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.randint(low=0, high=2, size=(8, 8))\n",
    "mask = mask > 0\n",
    "x = torch.rand((4, 8, 20))\n",
    "attn_output, _ = mha(x, x, x, attn_mask=mask, need_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d6c7f029-6ce2-41b6-9104-b2043fc68a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128, 128])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677129db-2c42-4cc1-8918-e726b1b9607f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
