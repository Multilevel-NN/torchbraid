{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98804907",
   "metadata": {},
   "source": [
    "### Apply layer-parallel Torchbraid to simple UCI HAR problem\n",
    "The dataset collates smartphone motion sensor data for a group of 30 volunteers while each person performed six different activites (walking, walking upstairs, walking downstairs, sitting, standing, and laying). The goal of the learning here is to predict the action based on the time-series acceleration and velocity data.\n",
    "\n",
    "The recurrent neural network used here is a layer-parallel version of a GRU network, developed in this paper:\n",
    "```\n",
    "Moon, Gordon Euhyun, and Eric C. Cyr. \"Parallel Training of GRU Networks with a Multi-Grid Solver for Long\n",
    "Sequences.\" arXiv preprint arXiv:2203.04738 (2022).\n",
    "```\n",
    "\n",
    "### Preliminaries\n",
    "- See `examples/mnist/start0_install_mpi_notebook.ipynb`, and `examples/mnist/start1_simple_mpi_notebook.ipynb` for setting up MPI-compatible Jupyter installation\n",
    "- Recommended to start ipython cluster for default notebook settings with (assumes 4 cores)\n",
    "\n",
    "        $ ipcluster start --n=4 --engines=mpi --profile=mpi\n",
    "\n",
    "  or\n",
    "\n",
    "        $ ipcluster start --n=4 --engines=MPIEngineSetLauncher --profile=mpi\n",
    "        \n",
    "#### Layer-parallel runs most efficiently when the \n",
    "\n",
    "    (number of processors)*(coarsening factor) = k*(number of ResNet Layers)\n",
    "    \n",
    "where `k` is some integer. This experiment is designed to satisfy this with the default parameters and four processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59da843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local ipython cluster.  Note, the ipcluster profile name must match the\n",
    "# below named profile. Here, we use 'mpi', but you can name the cluster profile anything\n",
    "from ipyparallel import Client, error\n",
    "cluster = Client(profile='mpi')\n",
    "print('profile:', cluster.profile)\n",
    "print(\"IDs:\", cluster.ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc80ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Must point to your Torchbraid location, used for location to download MNIST dataset\n",
    "torchbraid_path = \"/path/to/torchbraid\"\n",
    "\n",
    "# If using Makefile Torchbraid install, must uncomment to update system path to Torchbraid \n",
    "#import sys\n",
    "#sys.path.append(torchbraid_path + \"/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018636b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from __future__ import print_function\n",
    "\n",
    "import statistics as stats\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from mpi4py import MPI\n",
    "\n",
    "import torchbraid\n",
    "import torchbraid.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# MPI information\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "procs = comm.Get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f805aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "example_path = torchbraid_path + '/examples/UCI_HAR_Recurrent'\n",
    "dataset_path = example_path + '/UCI HAR Dataset'\n",
    "\n",
    "# If the data hasn't been downloaded yet, download it on rank 0 only\n",
    "if (rank == 0) and (not os.path.exists(dataset_path)):\n",
    "  import zipfile\n",
    "  import urllib\n",
    "  import shutil\n",
    "    \n",
    "  # Link to the UCI database\n",
    "  download_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
    "  # Download the zip\n",
    "  download_filename, _ = urllib.request.urlretrieve(download_url, example_path + '/UCIHARDataset.zip')\n",
    "  # Upzip the file\n",
    "  with zipfile.ZipFile(download_filename, 'r') as downloaded_zip:\n",
    "    downloaded_zip.extractall(path=example_path)\n",
    "  # Do some cleanup\n",
    "  os.remove(download_filename)\n",
    "  shutil.rmtree(example_path + '/__MACOSX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f5ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def imp_gru_cell_fast(dt : float, x_red_r : torch.Tensor, x_red_z : torch.Tensor, x_red_n : torch.Tensor, \n",
    "                      h : torch.Tensor, lin_rh_W : torch.Tensor, lin_zh_W : torch.Tensor,\n",
    "                      lin_nr_W : torch.Tensor, lin_nr_b : torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "  r   =    torch.sigmoid(x_red_r +     F.linear(h,lin_rh_W))\n",
    "  n   =    torch.   tanh(x_red_n + r * F.linear(h,lin_nr_W, lin_nr_b))\n",
    "  dtz = dt*torch.sigmoid(x_red_z +     F.linear(h,lin_zh_W))\n",
    "\n",
    "  return torch.div(torch.addcmul(h,dtz,n),1.0+dtz)\n",
    "\n",
    "def imp_gru_cell(dt : float, x : torch.Tensor, h : torch.Tensor,\n",
    "                 lin_rx_W : torch.Tensor, lin_rx_b : torch.Tensor, lin_rh_W : torch.Tensor,\n",
    "                 lin_zx_W : torch.Tensor, lin_zx_b : torch.Tensor, lin_zh_W : torch.Tensor,\n",
    "                 lin_nx_W : torch.Tensor, lin_nx_b : torch.Tensor, lin_nr_W : torch.Tensor, \n",
    "                 lin_nr_b : torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "  r   =      torch.sigmoid(F.linear(x, lin_rx_W, lin_rx_b) +     F.linear(h, lin_rh_W))\n",
    "  n   =      torch.   tanh(F.linear(x, lin_nx_W, lin_nx_b) + r * F.linear(h, lin_nr_W,lin_nr_b))\n",
    "  dtz = dt * torch.sigmoid(F.linear(x, lin_zx_W, lin_zx_b) +     F.linear(h, lin_zh_W))\n",
    "\n",
    "  return torch.div(torch.addcmul(h, dtz, n), 1.0 + dtz)\n",
    "\n",
    "\n",
    "class ImplicitGRUBlock(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(ImplicitGRUBlock, self).__init__()\n",
    "\n",
    "    #\n",
    "\n",
    "    self.lin_rx = [None,None]\n",
    "    self.lin_rh = [None,None]\n",
    "    self.lin_rx[0] = nn.Linear(input_size, hidden_size, True)\n",
    "    self.lin_rh[0] = nn.Linear(hidden_size, hidden_size, False)\n",
    "\n",
    "    self.lin_zx = [None,None]\n",
    "    self.lin_zh = [None,None]\n",
    "    self.lin_zx[0] = nn.Linear(input_size, hidden_size, True)\n",
    "    self.lin_zh[0] = nn.Linear(hidden_size, hidden_size, False)\n",
    "\n",
    "    self.lin_nx = [None,None]\n",
    "    self.lin_nr = [None,None]\n",
    "    self.lin_nx[0] = nn.Linear(input_size, hidden_size, True)\n",
    "    self.lin_nr[0] = nn.Linear(hidden_size, hidden_size, True)\n",
    "\n",
    "    #\n",
    "\n",
    "    self.lin_rx[1] = nn.Linear(hidden_size, hidden_size, True)\n",
    "    self.lin_rh[1] = nn.Linear(hidden_size, hidden_size, False)\n",
    "\n",
    "    self.lin_zx[1] = nn.Linear(hidden_size, hidden_size, True)\n",
    "    self.lin_zh[1] = nn.Linear(hidden_size, hidden_size, False)\n",
    "\n",
    "    self.lin_nx[1] = nn.Linear(hidden_size, hidden_size, True)\n",
    "    self.lin_nr[1] = nn.Linear(hidden_size, hidden_size, True)\n",
    "\n",
    "    # record the layers so that they are handled by backprop correctly\n",
    "    layers =  self.lin_rx + self.lin_rh + \\\n",
    "              self.lin_zx + self.lin_zh + \\\n",
    "              self.lin_nx + self.lin_nr\n",
    "    self.lin_layers = nn.ModuleList(layers)\n",
    "\n",
    "  def reduceX(self, x):\n",
    "    x_red_r = self.lin_rx[0](x)\n",
    "    x_red_z = self.lin_zx[0](x)\n",
    "    x_red_n = self.lin_nx[0](x)\n",
    "\n",
    "    return (x_red_r, x_red_z, x_red_n)\n",
    "\n",
    "  def fastForward(self, level, tstart, tstop, x_red, h_prev):\n",
    "    dt = tstop-tstart\n",
    "\n",
    "    h_prev = h_prev[0]\n",
    "    h0 = imp_gru_cell_fast(dt, *x_red, h_prev[0],\n",
    "                           self.lin_rh[0].weight,\n",
    "                           self.lin_zh[0].weight,\n",
    "                           self.lin_nr[0].weight, self.lin_nr[0].bias)\n",
    "    h1 = imp_gru_cell(dt, h0, h_prev[1],\n",
    "                      self.lin_rx[1].weight, self.lin_rx[1].bias, self.lin_rh[1].weight,\n",
    "                      self.lin_zx[1].weight, self.lin_zx[1].bias, self.lin_zh[1].weight,\n",
    "                      self.lin_nx[1].weight, self.lin_nx[1].bias, self.lin_nr[1].weight, self.lin_nr[1].bias)\n",
    "\n",
    "    # Note: we return a tuple with a single element\n",
    "    return (torch.stack((h0, h1)), )\n",
    "\n",
    "\n",
    "  def forward(self, level, tstart, tstop, x, h_prev):\n",
    "    dt = tstop-tstart\n",
    "\n",
    "    h_prev = h_prev[0]\n",
    "    h0 = imp_gru_cell(dt, x, h_prev[0],\n",
    "                      self.lin_rx[0].weight, self.lin_rx[0].bias, self.lin_rh[0].weight,\n",
    "                      self.lin_zx[0].weight, self.lin_zx[0].bias, self.lin_zh[0].weight,\n",
    "                      self.lin_nx[0].weight, self.lin_nx[0].bias, self.lin_nr[0].weight, self.lin_nr[0].bias)\n",
    "    h1 = imp_gru_cell(dt, h0, h_prev[1],\n",
    "                      self.lin_rx[1].weight, self.lin_rx[1].bias, self.lin_rh[1].weight,\n",
    "                      self.lin_zx[1].weight, self.lin_zx[1].bias, self.lin_zh[1].weight,\n",
    "                      self.lin_nx[1].weight, self.lin_nx[1].bias, self.lin_nr[1].weight, self.lin_nr[1].bias)\n",
    "\n",
    "    # Note: we return a tuple with a single element\n",
    "    return (torch.stack((h0, h1)), )\n",
    "\n",
    "class CloseLayer(nn.Module):\n",
    "  def __init__(self, hidden_size, num_classes):\n",
    "    super(CloseLayer, self).__init__()\n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Parallel network class, primarily builds a RNN_Parallel network object\n",
    "# num_layers: number of layers per processor\n",
    "# for definitions of layer-parallel (and other parameters) see more advanced scripts and notebooks\n",
    "class ParallelNet(nn.Module):\n",
    "  def __init__(self,\n",
    "               input_size=9,\n",
    "               hidden_size=100,\n",
    "               Tf=None,\n",
    "               num_layers=2,\n",
    "               num_classes=6,\n",
    "               num_steps=32,\n",
    "               max_levels=3,\n",
    "               max_iters=1,\n",
    "               fwd_max_iters=2,\n",
    "               print_level=0,\n",
    "               braid_print_level=0,\n",
    "               cfactor=4,\n",
    "               skip_downcycle=True):\n",
    "    super(ParallelNet, self).__init__()\n",
    "\n",
    "    self.RNN_model = ImplicitGRUBlock(input_size, hidden_size)\n",
    "\n",
    "    if Tf == None:\n",
    "      Tf = float(num_steps) * MPI.COMM_WORLD.Get_size() # when using an implicit method with GRU, \n",
    "    self.Tf = Tf\n",
    "    self.dt = Tf / float(num_steps * MPI.COMM_WORLD.Get_size())\n",
    "\n",
    "    self.parallel_rnn = torchbraid.RNN_Parallel(MPI.COMM_WORLD,\n",
    "                                                self.RNN_model,\n",
    "                                                num_steps,hidden_size,num_layers,\n",
    "                                                Tf,\n",
    "                                                max_fwd_levels=max_levels,\n",
    "                                                max_bwd_levels=max_levels,\n",
    "                                                max_iters=max_iters)\n",
    "\n",
    "    if fwd_max_iters > 0:\n",
    "      self.parallel_rnn.setFwdMaxIters(fwd_max_iters)\n",
    "\n",
    "    self.parallel_rnn.setPrintLevel(print_level)\n",
    "\n",
    "    cfactor_dict = dict()\n",
    "    cfactor_dict[-1] = cfactor\n",
    "    self.parallel_rnn.setCFactor(cfactor_dict)\n",
    "    self.parallel_rnn.setSkipDowncycle(skip_downcycle)\n",
    "    self.parallel_rnn.setNumRelax(1)            # FCF on all levels, by default\n",
    "    self.parallel_rnn.setFwdNumRelax(1, level=0) # F-Relaxation on the fine grid (by default)\n",
    "    self.parallel_rnn.setBwdNumRelax(0, level=0) # F-Relaxation on the fine grid (by default)\n",
    "\n",
    "    # this object ensures that only the RNN_Parallel code runs on ranks!=0\n",
    "    compose = self.compose = self.parallel_rnn.comp_op()\n",
    "\n",
    "    self.close_rnn = compose(CloseLayer, hidden_size, num_classes)\n",
    "\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "  def forward(self, x):\n",
    "    h = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "    hn = self.parallel_rnn(x, h)\n",
    "\n",
    "    x = self.compose(self.close_rnn,hn[-1,:,:])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Train model for one epoch\n",
    "# Return values: per batch losses and training times, model parameters updated in-place\n",
    "def train(rank, params, model, train_loader, optimizer, epoch, compose, device):\n",
    "  train_times = []\n",
    "  losses = []\n",
    "  model.train()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  total_time = 0.0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    start_time = timer()\n",
    "    data, taraget = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = compose(criterion, output, target)\n",
    "    loss.backward()\n",
    "    stop_time = timer()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_time += stop_time - start_time\n",
    "    train_times.append(stop_time - start_time)\n",
    "    losses.append(loss.item())\n",
    "    if batch_idx % 10 == 0:\n",
    "      root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "               100. * batch_idx / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))\n",
    "\n",
    "  root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "           100. * (batch_idx + 1) / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))\n",
    "  return losses, train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb56f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Evaluate model on validation data\n",
    "# Return: number of correctly classified test items, total number of test items, loss on test data set\n",
    "def test(rank, model, test_loader, compose, device):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  with torch.no_grad():\n",
    "    for data,target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      test_loss += compose(criterion, output, target).item()\n",
    "\n",
    "      if rank == 0:\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "\n",
    "  root_print(rank, '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  return correct, len(test_loader.dataset), test_loss\n",
    "\n",
    "# Parallel printing helper function\n",
    "def root_print(rank, s):\n",
    "  if rank == 0:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Set a few training parameters  \n",
    "params = {}\n",
    "params['percent_data'] = 1.0     # how much of the data to read in and use for training/testing\n",
    "params['batch_size'] = 100       # input batch size for training\n",
    "params['epochs'] = 5             # number of epochs to train (for quick example)\n",
    "#params['epochs'] = 12           # longer training, but improved results\n",
    "params['lr'] = 0.01              # learning rate\n",
    "params['sequence_length'] = 128  # number of samples in a single sequence (determined by the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3e5f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "# Use device or CPU?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device, host = torchbraid.utils.getDevice(comm=comm)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Run info rank: {rank}: | Device: {device} | Host: {host}')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Compute number of sample from the sequence is assigned to each processor\n",
    "local_steps = int(params['sequence_length'] / procs)\n",
    "if params['sequence_length'] % procs != 0:\n",
    "  root_print(rank, f\"Number of processors ({procs}) must be a perfect divisor of sequence length ({params['sequence_length']}).\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Custom DataLoader for distributing sequences across ranks\n",
    "class ParallelRNNDataLoader(torch.utils.data.DataLoader):\n",
    "  def __init__(self, comm, dataset, batch_size, shuffle=False):\n",
    "    self.dataset = dataset\n",
    "    self.shuffle = shuffle\n",
    "    \n",
    "    rank = comm.Get_rank()\n",
    "    num_procs = comm.Get_size()\n",
    "\n",
    "    if comm.Get_rank() == 0:\n",
    "      # build a gneerator to build one master initial seed\n",
    "      serial_generator = torch.Generator()\n",
    "      self.initial_seed = serial_generator.initial_seed()\n",
    "    else:\n",
    "      self.serial_loader = None\n",
    "      self.initial_seed = None\n",
    "\n",
    "    # distribute the initial seed\n",
    "    self.initial_seed = comm.bcast(self.initial_seed, root=0)\n",
    "\n",
    "    # break up sequences\n",
    "    x_block = [[] for n in range(num_procs)]\n",
    "    y_block = [[] for n in range(num_procs)]\n",
    "    if rank == 0:\n",
    "      sz = len(dataset)\n",
    "      for i in range(sz):\n",
    "        x,y = dataset[i]\n",
    "        x_split = torch.chunk(x, num_procs, dim=0)\n",
    "        y_split = num_procs*[y]\n",
    "\n",
    "        for p,(x_in, y_in) in enumerate(zip(x_split, y_split)):\n",
    "          x_block[p].append(x_in)\n",
    "          y_block[p].append(y_in)\n",
    "\n",
    "      for p,(x, y) in enumerate(zip(x_block, y_block)):\n",
    "        x_block[p] = torch.stack(x)\n",
    "        y_block[p] = torch.stack(y)\n",
    "\n",
    "    x_local = comm.scatter(x_block, root=0)\n",
    "    y_local = comm.scatter(y_block, root=0)\n",
    "\n",
    "    self.parallel_dataset = torch.utils.data.TensorDataset(x_local, y_local)\n",
    "\n",
    "    # now setup the parallel loader\n",
    "    if shuffle==True:\n",
    "      parallel_generator = torch.Generator()\n",
    "      parallel_generator.manual_seed(self.initial_seed)\n",
    "      sampler = torch.utils.data.sampler.RandomSampler(self.parallel_dataset, generator=parallel_generator)\n",
    "      torch.utils.data.DataLoader.__init__(self, self.parallel_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    else:\n",
    "      torch.utils.data.DataLoader.__init__(self, self.parallel_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b91cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Read in UCI_HAR \n",
    "x_data = {}\n",
    "y_data = {}\n",
    "\n",
    "for type_str in ['train', 'test']:\n",
    "  d_path = dataset_path + '/' + type_str\n",
    "  i_path = d_path + '/Inertial Signals/'\n",
    "\n",
    "  # load label data\n",
    "  y = np.loadtxt('%s/y_%s.txt' % (d_path, type_str))\n",
    "\n",
    "  # give upu on the pythonic way\n",
    "  y_data[type_str] = torch.tensor([int(y[i]-1) for i in range(y.shape[0])], dtype=torch.long)\n",
    "\n",
    "  # load feature data\n",
    "  body_x = np.loadtxt('%s/body_acc_x_%s.txt' % (i_path, type_str))\n",
    "  body_y = np.loadtxt('%s/body_acc_y_%s.txt' % (i_path, type_str))\n",
    "  body_z = np.loadtxt('%s/body_acc_z_%s.txt' % (i_path, type_str))\n",
    "\n",
    "  gyro_x = np.loadtxt('%s/body_gyro_x_%s.txt' % (i_path, type_str))\n",
    "  gyro_y = np.loadtxt('%s/body_gyro_y_%s.txt' % (i_path, type_str))\n",
    "  gyro_z = np.loadtxt('%s/body_gyro_z_%s.txt' % (i_path, type_str))\n",
    "\n",
    "  totl_x = np.loadtxt('%s/total_acc_x_%s.txt' % (i_path, type_str))\n",
    "  totl_y = np.loadtxt('%s/total_acc_y_%s.txt' % (i_path, type_str))\n",
    "  totl_z = np.loadtxt('%s/total_acc_z_%s.txt' % (i_path, type_str))\n",
    "\n",
    "  x_data[type_str] = torch.Tensor(np.stack([body_x, body_y, body_z,\n",
    "                                            gyro_x, gyro_y, gyro_z,\n",
    "                                            totl_x, totl_y, totl_z], axis=2))\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(x_data['train'], y_data['train'])\n",
    "test_set  = torch.utils.data.TensorDataset(x_data['test'], y_data['test'])\n",
    "\n",
    "train_size = int(7352 * params['percent_data'])\n",
    "test_size = int(2947 * params['percent_data'])\n",
    "train_set = torch.utils.data.Subset(train_set, range(train_size))\n",
    "test_set = torch.utils.data.Subset(test_set, range(test_size))\n",
    "\n",
    "train_loader = ParallelRNNDataLoader(comm, dataset=train_set, batch_size=params['batch_size'], shuffle=True)\n",
    "test_loader = ParallelRNNDataLoader(comm, dataset=test_set, batch_size=params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba553705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Create layer-parallel network\n",
    "# Note this can be done on only one processor, but will be slow\n",
    "\n",
    "model = ParallelNet(num_steps=local_steps).to(device)\n",
    "\n",
    "# Declare optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9)\n",
    "\n",
    "# Carry out parallel training\n",
    "batch_losses = []; batch_times = []\n",
    "epoch_times = []; test_times = []\n",
    "validat_correct_counts = []\n",
    "\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "  start_time = timer()\n",
    "  [losses, train_times] = train(rank=rank, params=params, model=model, train_loader=train_loader, \n",
    "                                optimizer=optimizer, epoch=epoch, compose=model.compose, device=device)\n",
    "  epoch_times += [timer() - start_time]\n",
    "  batch_losses += losses\n",
    "  batch_times += train_times\n",
    "\n",
    "  start_time = timer()\n",
    "  validat_correct, validat_size, validat_loss = test(rank=rank, model=model, test_loader=test_loader, \n",
    "                                                     compose=model.compose, device=device)\n",
    "  test_times += [timer() - start_time]\n",
    "  validat_correct_counts += [validat_correct]\n",
    "\n",
    "root_print(rank,\n",
    "           f'TIME PER EPOCH: {\"{:.2f}\".format(stats.mean(epoch_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f})\".format(stats.mean(epoch_times))) if len(epoch_times) > 1 else \"\"}')\n",
    "root_print(rank,\n",
    "           f'TIME PER TEST:  {\"{:.2f}\".format(stats.mean(test_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f})\".format(stats.mean(test_times))) if len(test_times) > 1 else \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576afc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the loss, validation \n",
    "root_print(rank, f'\\nMin batch time:   {\"{:.3f}\".format(np.min(batch_times))} ')\n",
    "root_print(rank, f'Mean batch time:  {\"{:.3f}\".format(stats.mean(batch_times))} ')\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "  fig, ax1 = plt.subplots()\n",
    "  plt.title('Layer-parallel run with %d processors\\n UCI HAR dataset'%(procs), fontsize=15)\n",
    "  ax1.plot(batch_losses, color='b', linewidth=2)\n",
    "  ax1.grid(True, color='k', linestyle='-', linewidth=0.4)\n",
    "  ax1.set_xlabel(r\"Batch number\", fontsize=13)\n",
    "  ax1.set_ylabel(r\"Loss\", fontsize=13, color='b')\n",
    "  \n",
    "  ax2 = ax1.twinx()\n",
    "  epoch_points = np.arange(1, len(validat_correct_counts) + 1) * len(train_loader)\n",
    "  validation_percentage = np.array(validat_correct_counts) / validat_size\n",
    "  ax2.plot( epoch_points, validation_percentage, color='r', linestyle='dashed', linewidth=2, marker='o')\n",
    "  ax2.set_ylabel(r\"Validation rate\", fontsize=13, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157f6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
