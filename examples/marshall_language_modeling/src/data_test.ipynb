{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7168735e-c617-4e5e-a9ad-9d0e01637f93",
   "metadata": {},
   "source": [
    "# Notebook to play with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cecece31-2918-4aa4-8ba4-078aef8df5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import statistics as stats\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchbraid\n",
    "import torchbraid.utils\n",
    "from torchvision import datasets, transforms\n",
    "import sys\n",
    "\n",
    "from network_architecture import parse_args, ParallelNet\n",
    "from mpi4py import MPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7eed9de-f23e-468f-9466-ff1a02bb913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partially taken from Karpathy's github: [url]\n",
    "\n",
    "import os\n",
    "import torch\n",
    "# from transformers import GPT2Tokenizer, GPT2Model  <-- below\n",
    "\n",
    "def obtain_data(data_dir, input_text, tokenization):\n",
    "  data_path = os.path.join(data_dir, input_text + '.txt')\n",
    "\n",
    "  print('1.1 Reading text')\n",
    "  with open(data_path, 'r', encoding='utf-8') as f:\n",
    "      text = f.read()\n",
    "\n",
    "  if tokenization == 'character':\n",
    "    print('1.2 Building character-level tokenizer')\n",
    "    # here are all the unique characters that occur in this text\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "    # create a mapping from characters to integers\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "    decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "    print('1.3 Encoding data')\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "  elif tokenization == 'gpt2':\n",
    "    from transformers import GPT2Tokenizer\n",
    "\n",
    "    print('1.2 Obtaining gpt2 tokenizer')\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2-tokenizer')\n",
    "    # tokenizer.pad_token = '<pad>'\n",
    "    decode = tokenizer.decode\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    print('1.3 Encoding data')\n",
    "    data = tokenizer(text)['input_ids']\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    print(data.shape)\n",
    "\n",
    "  else: raise Exception()\n",
    "\n",
    "  print('1.4 Splitting data into training and validation data')\n",
    "  n = int(.9*len(data))\n",
    "  train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "  return train_data, val_data, decode, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cc5b200-47f2-4a08-805c-4e0fa5719975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Reading text\n",
      "1.2 Obtaining gpt2 tokenizer\n",
      "1.3 Encoding data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([338025])\n",
      "1.4 Splitting data into training and validation data\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, decode, vocabulary_size = \\\n",
    "    obtain_data('.', 'shakespeare', 'gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb867175-9e30-4213-b854-34f6eda687ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNoble Sebastian,\\nThou let'st thy fortune sleep--die, rather; wink'st\\nWhiles thou art waking.\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(val_data[-30:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cab88a6e-a370-409f-8d64-bd59761ddbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5962, 22307,    25,  ...,  3398,  9399,    25])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aa30c6c5-a9ea-491c-a1f4-78c067ab4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, context_window, batch_size, device):\n",
    "  ix = torch.randint(len(data) - context_window, (batch_size,))\n",
    "  x = torch.stack([data[i : i + context_window] for i in ix])\n",
    "  y = torch.stack([data[i+1 : i+1 + context_window] for i in ix])\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  return x, y\n",
    "\n",
    "x, y = get_batch(train_data, 256, 32, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb995e6b-a963-41bc-bbdc-9116e16c89b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0aedc6-a3dd-4932-a93f-604570171825",
   "metadata": {},
   "source": [
    "# Okay I think the data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99a1fab3-9575-4d9d-96eb-a0843ec5b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73725540-2105-4548-9175-47be915ee7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 256\n",
    "batch_size = 32\n",
    "\n",
    "# Total number of batches; we should decrease by 1 just in case for shifting\n",
    "num_samples_per_epoch_train = context_window * (len(train_data) // context_window - 1) \n",
    "num_samples_per_epoch_val = context_window * (len(val_data) // context_window - 1) \n",
    "\n",
    "train_data[0:num_samples_per_epoch_train].reshape((context_window, -1))\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, context_window = 256):\n",
    "        self.length = len(data) // context_window - 1\n",
    "\n",
    "        self.data = data\n",
    "        self.context_window = context_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx * self.context_window:(idx + 1) * self.context_window], \\\n",
    "                self.data[1 + idx * self.context_window:1 + (idx + 1) * self.context_window]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3978d8b9-b4bf-4732-a207-7ebdb933793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74b0cdd3-27df-42da-9cf6-b12ceb17518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "04c017dc-be56-4684-819e-60260db245c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a335feae-4d71-4c27-86a6-107218f40404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6a55c361-8286-496c-893a-3d7a71d11c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dee01aff-2c7c-40dc-a851-f196cf155d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fdf00c3-3b76-4335-be91-38c04cb17565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5962, 22307,    25,  ...,    11,   198,  9590],\n",
       "        [ 6508, 27794,   262,  ...,  1867,   338,   511],\n",
       "        [ 6095,    30,   198,  ...,    32,  1295,  2174],\n",
       "        ...,\n",
       "        [  314, 12472,    13,  ...,     0, 44012, 13676],\n",
       "        [  428, 35831,   594,  ...,   198,  1135,   481],\n",
       "        [  467,  2513,   257,  ...,   345, 12891,    25]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:\n",
    "    context_window * (len(train_data) // context_window)\n",
    "].reshape((context_window, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3bbb2e8-602d-4953-ae8a-7b9fdd4aaed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304222"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0:\n",
    "    context_window * len(train_data) // context_window\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c483b32-c282-497a-a6fc-5037405ec44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304128"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_window * (len(train_data) // context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a50c6623-d918-4eb2-8d3d-ae81babd5eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1188.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "304128 / context_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c519e2-6090-4af8-b0e1-ec67d057fe65",
   "metadata": {},
   "source": [
    "# Take care of wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c07c644-82fa-492c-98aa-e1d09831d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-tokenizer')\n",
    "# tokenzer.pad_token = '<pad>'\n",
    "decode = tokenizer.decode\n",
    "vocab_size = tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ad24375-3ac0-4cae-9ca3-1bdc6c61e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "356492fb-5f2a-412e-baa9-edccd1fb66d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b332f52369da4df6af81f7a553f86ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Wikipedia:   0%|          | 0/2966378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "counter = 0 \n",
    "# Counted number of lines on linux to be 2966378\n",
    "with open('../data/wikipedia.txt', 'r', encoding='utf-8') as f:\n",
    "  # Use tqdm to iterate through lines with a description\n",
    "  for line in tqdm(f, desc=\"Tokenizing Wikipedia\", total=2966378):\n",
    "    text = line.strip()  # Strip whitespace from each line\n",
    "\n",
    "    # Check if it's a blank line (after stripping)\n",
    "    if not text:\n",
    "      continue\n",
    "\n",
    "    # Tokenize and process the text\n",
    "    data += tokenizer(text)['input_ids']\n",
    "    # counter += 1\n",
    "    # print(text)\n",
    "\n",
    "    # if counter == 7: \n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab738e64-d15f-4149-97b1-e2535dbdb676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= Valkyria Chronicles III =Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6116059-bdcb-4854-b0d4-47d003fd78e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
