Using GPU Device
Run info rank: 0: Torch version: 2.1.2+cu121 | Device: cuda:0 | Host: cpu
Loading dataset; args.percent_data=0.2
Loaded tensor from ../data/wikipedia.data
1.4 Splitting data into training and validation data
len(train_data)=104035585, len(val_data)=11559510 percent_data=0.2
len(train_data)=20807117, len(val_data)=2311902
-- procs    = 1
-- Tf       = 1.0
-- steps    = 64
-- max_levels     = 1
-- max_bwd_iters  = 1
-- max_fwd_iters  = 3
-- cfactor        = 4
-- fine fcf       = False
-- skip down      = True

args.model_dimension=384 args.num_heads=6 args.batch_size=64
rank 0: len(list(model.parameters())) 1798
rank 0: 152238673
Train Epoch: 1 [0/81276 (0%)]	Loss: 11.001037 LR 6.00e-07
	 Magnitude of data 1.348e+06 output 1.660e+04 target 1.348e+06
Train Epoch: 1 [640/81276 (1%)]	Loss: 11.003200 LR 6.60e-06
	 Magnitude of data 1.314e+06 output 1.659e+04 target 1.314e+06
Train Epoch: 1 [1280/81276 (2%)]	Loss: 10.959468 LR 1.26e-05
	 Magnitude of data 1.495e+06 output 1.657e+04 target 1.495e+06
Train Epoch: 1 [1920/81276 (2%)]	Loss: 10.911312 LR 1.86e-05
	 Magnitude of data 1.218e+06 output 1.654e+04 target 1.218e+06
Train Epoch: 1 [2560/81276 (3%)]	Loss: 10.850140 LR 2.46e-05
	 Magnitude of data 1.281e+06 output 1.651e+04 target 1.281e+06
Train Epoch: 1 [3200/81276 (4%)]	Loss: 10.781753 LR 3.06e-05
	 Magnitude of data 1.317e+06 output 1.649e+04 target 1.317e+06
Train Epoch: 1 [3840/81276 (5%)]	Loss: 10.631322 LR 3.66e-05
	 Magnitude of data 1.281e+06 output 1.651e+04 target 1.281e+06
Train Epoch: 1 [4480/81276 (6%)]	Loss: 10.426159 LR 4.26e-05
	 Magnitude of data 1.306e+06 output 1.676e+04 target 1.306e+06
Train Epoch: 1 [5120/81276 (6%)]	Loss: 10.091616 LR 4.86e-05
	 Magnitude of data 1.360e+06 output 1.790e+04 target 1.360e+06
Train Epoch: 1 [5760/81276 (7%)]	Loss: 9.619864 LR 5.46e-05
	 Magnitude of data 1.349e+06 output 2.122e+04 target 1.350e+06
Train Epoch: 1 [6400/81276 (8%)]	Loss: 9.191680 LR 6.06e-05
	 Magnitude of data 1.285e+06 output 2.627e+04 target 1.286e+06
Train Epoch: 1 [7040/81276 (9%)]	Loss: 8.861720 LR 6.66e-05
	 Magnitude of data 1.278e+06 output 3.161e+04 target 1.278e+06
Train Epoch: 1 [7680/81276 (9%)]	Loss: 8.626253 LR 7.26e-05
	 Magnitude of data 1.419e+06 output 3.693e+04 target 1.419e+06
Train Epoch: 1 [8320/81276 (10%)]	Loss: 8.294304 LR 7.86e-05
	 Magnitude of data 1.234e+06 output 4.232e+04 target 1.234e+06
Train Epoch: 1 [8960/81276 (11%)]	Loss: 8.102690 LR 8.46e-05
	 Magnitude of data 1.348e+06 output 4.773e+04 target 1.348e+06
Train Epoch: 1 [9600/81276 (12%)]	Loss: 7.885823 LR 9.06e-05
	 Magnitude of data 1.364e+06 output 5.301e+04 target 1.364e+06
Train Epoch: 1 [10240/81276 (13%)]	Loss: 7.711313 LR 9.66e-05
	 Magnitude of data 1.276e+06 output 5.796e+04 target 1.276e+06
Train Epoch: 1 [10880/81276 (13%)]	Loss: 7.609297 LR 1.03e-04
	 Magnitude of data 1.240e+06 output 6.264e+04 target 1.240e+06
Train Epoch: 1 [11520/81276 (14%)]	Loss: 7.604296 LR 1.09e-04
	 Magnitude of data 1.342e+06 output 6.696e+04 target 1.342e+06
Train Epoch: 1 [12160/81276 (15%)]	Loss: 7.559593 LR 1.15e-04
	 Magnitude of data 1.359e+06 output 7.088e+04 target 1.359e+06
Train Epoch: 1 [12800/81276 (16%)]	Loss: 7.591958 LR 1.21e-04
	 Magnitude of data 1.366e+06 output 7.446e+04 target 1.366e+06
Train Epoch: 1 [13440/81276 (17%)]	Loss: 7.387027 LR 1.27e-04
	 Magnitude of data 1.241e+06 output 7.760e+04 target 1.241e+06
Train Epoch: 1 [14080/81276 (17%)]	Loss: 7.499989 LR 1.33e-04
	 Magnitude of data 1.396e+06 output 8.058e+04 target 1.396e+06
Train Epoch: 1 [14720/81276 (18%)]	Loss: 7.564475 LR 1.38e-04
	 Magnitude of data 1.450e+06 output 8.273e+04 target 1.450e+06
Train Epoch: 1 [15360/81276 (19%)]	Loss: 7.596522 LR 1.44e-04
	 Magnitude of data 1.387e+06 output 8.424e+04 target 1.387e+06
Train Epoch: 1 [16000/81276 (20%)]	Loss: 7.408011 LR 1.50e-04
	 Magnitude of data 1.371e+06 output 8.354e+04 target 1.371e+06
Train Epoch: 1 [16640/81276 (20%)]	Loss: 7.228491 LR 1.56e-04
	 Magnitude of data 1.274e+06 output 8.388e+04 target 1.274e+06
Train Epoch: 1 [17280/81276 (21%)]	Loss: 7.464882 LR 1.62e-04
	 Magnitude of data 1.443e+06 output 8.546e+04 target 1.442e+06
Train Epoch: 1 [17920/81276 (22%)]	Loss: 7.211906 LR 1.68e-04
	 Magnitude of data 1.267e+06 output 8.694e+04 target 1.266e+06
Train Epoch: 1 [18560/81276 (23%)]	Loss: 6.974551 LR 1.74e-04
	 Magnitude of data 1.246e+06 output 8.852e+04 target 1.246e+06
Train Epoch: 1 [19200/81276 (24%)]	Loss: 6.996538 LR 1.80e-04
	 Magnitude of data 1.254e+06 output 8.977e+04 target 1.254e+06
Train Epoch: 1 [19840/81276 (24%)]	Loss: 7.134825 LR 1.86e-04
	 Magnitude of data 1.331e+06 output 9.321e+04 target 1.331e+06
Train Epoch: 1 [20480/81276 (25%)]	Loss: 7.020609 LR 1.92e-04
	 Magnitude of data 1.190e+06 output 9.437e+04 target 1.190e+06
Train Epoch: 1 [21120/81276 (26%)]	Loss: 7.053463 LR 1.98e-04
	 Magnitude of data 1.290e+06 output 9.379e+04 target 1.290e+06
Train Epoch: 1 [21760/81276 (27%)]	Loss: 7.070078 LR 2.04e-04
	 Magnitude of data 1.386e+06 output 9.649e+04 target 1.385e+06
Train Epoch: 1 [22400/81276 (28%)]	Loss: 6.990163 LR 2.10e-04
	 Magnitude of data 1.271e+06 output 9.868e+04 target 1.271e+06
Train Epoch: 1 [23040/81276 (28%)]	Loss: 7.143297 LR 2.16e-04
	 Magnitude of data 1.259e+06 output 9.867e+04 target 1.259e+06
Train Epoch: 1 [23680/81276 (29%)]	Loss: 6.882929 LR 2.22e-04
	 Magnitude of data 1.231e+06 output 1.023e+05 target 1.231e+06
Train Epoch: 1 [24320/81276 (30%)]	Loss: 7.037163 LR 2.28e-04
	 Magnitude of data 1.323e+06 output 1.020e+05 target 1.322e+06
Train Epoch: 1 [24960/81276 (31%)]	Loss: 6.751431 LR 2.34e-04
	 Magnitude of data 1.341e+06 output 1.017e+05 target 1.341e+06
Train Epoch: 1 [25600/81276 (32%)]	Loss: 6.884960 LR 2.40e-04
	 Magnitude of data 1.299e+06 output 1.043e+05 target 1.299e+06
Train Epoch: 1 [26240/81276 (32%)]	Loss: 6.947852 LR 2.46e-04
	 Magnitude of data 1.329e+06 output 1.055e+05 target 1.329e+06
Train Epoch: 1 [26880/81276 (33%)]	Loss: 6.651342 LR 2.52e-04
	 Magnitude of data 1.258e+06 output 1.055e+05 target 1.258e+06
Train Epoch: 1 [27520/81276 (34%)]	Loss: 6.912167 LR 2.58e-04
	 Magnitude of data 1.280e+06 output 1.077e+05 target 1.280e+06
Train Epoch: 1 [28160/81276 (35%)]	Loss: 6.850113 LR 2.64e-04
	 Magnitude of data 1.353e+06 output 1.066e+05 target 1.353e+06
Train Epoch: 1 [28800/81276 (35%)]	Loss: 6.780889 LR 2.70e-04
	 Magnitude of data 1.316e+06 output 1.098e+05 target 1.316e+06
Train Epoch: 1 [29440/81276 (36%)]	Loss: 6.857723 LR 2.76e-04
	 Magnitude of data 1.295e+06 output 1.075e+05 target 1.295e+06
Train Epoch: 1 [30080/81276 (37%)]	Loss: 6.748748 LR 2.82e-04
	 Magnitude of data 1.290e+06 output 1.117e+05 target 1.290e+06
Train Epoch: 1 [30720/81276 (38%)]	Loss: 6.740656 LR 2.88e-04
	 Magnitude of data 1.320e+06 output 1.127e+05 target 1.320e+06
Train Epoch: 1 [31360/81276 (39%)]	Loss: 6.635518 LR 2.94e-04
	 Magnitude of data 1.397e+06 output 1.130e+05 target 1.397e+06
Train Epoch: 1 [32000/81276 (39%)]	Loss: 6.749840 LR 2.99e-04
	 Magnitude of data 1.283e+06 output 1.175e+05 target 1.283e+06
Train Epoch: 1 [32640/81276 (40%)]	Loss: 6.919462 LR 2.99e-04
	 Magnitude of data 1.354e+06 output 1.131e+05 target 1.354e+06
Train Epoch: 1 [33280/81276 (41%)]	Loss: 6.799462 LR 2.99e-04
	 Magnitude of data 1.271e+06 output 1.155e+05 target 1.271e+06
Train Epoch: 1 [33920/81276 (42%)]	Loss: 6.775303 LR 2.99e-04
	 Magnitude of data 1.296e+06 output 1.218e+05 target 1.296e+06
Train Epoch: 1 [34560/81276 (43%)]	Loss: 6.780445 LR 2.99e-04
	 Magnitude of data 1.312e+06 output 1.185e+05 target 1.312e+06
Train Epoch: 1 [35200/81276 (43%)]	Loss: 7.094434 LR 2.99e-04
	 Magnitude of data 1.383e+06 output 1.187e+05 target 1.383e+06
Train Epoch: 1 [35840/81276 (44%)]	Loss: 6.671854 LR 2.99e-04
	 Magnitude of data 1.314e+06 output 1.178e+05 target 1.314e+06
Train Epoch: 1 [36480/81276 (45%)]	Loss: 6.709801 LR 2.99e-04
	 Magnitude of data 1.359e+06 output 1.170e+05 target 1.359e+06
Train Epoch: 1 [37120/81276 (46%)]	Loss: 6.387940 LR 2.98e-04
	 Magnitude of data 1.273e+06 output 1.197e+05 target 1.273e+06
Train Epoch: 1 [37760/81276 (46%)]	Loss: 6.499181 LR 2.98e-04
	 Magnitude of data 1.261e+06 output 1.211e+05 target 1.261e+06
Train Epoch: 1 [38400/81276 (47%)]	Loss: 6.490224 LR 2.98e-04
	 Magnitude of data 1.358e+06 output 1.231e+05 target 1.358e+06
Train Epoch: 1 [39040/81276 (48%)]	Loss: 6.653151 LR 2.98e-04
	 Magnitude of data 1.339e+06 output 1.291e+05 target 1.339e+06
Train Epoch: 1 [39680/81276 (49%)]	Loss: 6.362574 LR 2.98e-04
	 Magnitude of data 1.233e+06 output 1.244e+05 target 1.233e+06
Train Epoch: 1 [40320/81276 (50%)]	Loss: 6.675586 LR 2.98e-04
	 Magnitude of data 1.395e+06 output 1.221e+05 target 1.395e+06
Train Epoch: 1 [40960/81276 (50%)]	Loss: 6.545661 LR 2.98e-04
	 Magnitude of data 1.324e+06 output 1.254e+05 target 1.324e+06
Train Epoch: 1 [41600/81276 (51%)]	Loss: 6.568877 LR 2.98e-04
	 Magnitude of data 1.367e+06 output 1.260e+05 target 1.367e+06
Train Epoch: 1 [42240/81276 (52%)]	Loss: 6.544545 LR 2.98e-04
	 Magnitude of data 1.301e+06 output 1.333e+05 target 1.301e+06
Train Epoch: 1 [42880/81276 (53%)]	Loss: 6.330152 LR 2.98e-04
	 Magnitude of data 1.223e+06 output 1.311e+05 target 1.223e+06
Train Epoch: 1 [43520/81276 (54%)]	Loss: 6.477322 LR 2.98e-04
	 Magnitude of data 1.285e+06 output 1.317e+05 target 1.285e+06
Train Epoch: 1 [44160/81276 (54%)]	Loss: 6.432034 LR 2.98e-04
	 Magnitude of data 1.281e+06 output 1.356e+05 target 1.281e+06
Train Epoch: 1 [44800/81276 (55%)]	Loss: 6.496934 LR 2.98e-04
	 Magnitude of data 1.413e+06 output 1.350e+05 target 1.413e+06
Train Epoch: 1 [45440/81276 (56%)]	Loss: 6.549375 LR 2.98e-04
	 Magnitude of data 1.310e+06 output 1.356e+05 target 1.310e+06
Train Epoch: 1 [46080/81276 (57%)]	Loss: 6.587395 LR 2.98e-04
	 Magnitude of data 1.308e+06 output 1.368e+05 target 1.308e+06
Train Epoch: 1 [46720/81276 (58%)]	Loss: 6.596608 LR 2.98e-04
	 Magnitude of data 1.370e+06 output 1.374e+05 target 1.371e+06
Train Epoch: 1 [47360/81276 (58%)]	Loss: 6.281440 LR 2.97e-04
	 Magnitude of data 1.361e+06 output 1.350e+05 target 1.361e+06
Train Epoch: 1 [48000/81276 (59%)]	Loss: 6.550097 LR 2.97e-04
	 Magnitude of data 1.285e+06 output 1.417e+05 target 1.286e+06
Train Epoch: 1 [48640/81276 (60%)]	Loss: 6.251052 LR 2.97e-04
	 Magnitude of data 1.249e+06 output 1.378e+05 target 1.249e+06
Train Epoch: 1 [49280/81276 (61%)]	Loss: 6.314043 LR 2.97e-04
	 Magnitude of data 1.305e+06 output 1.350e+05 target 1.305e+06
Train Epoch: 1 [49920/81276 (61%)]	Loss: 6.435341 LR 2.97e-04
	 Magnitude of data 1.259e+06 output 1.388e+05 target 1.259e+06
Train Epoch: 1 [50560/81276 (62%)]	Loss: 6.409597 LR 2.97e-04
	 Magnitude of data 1.274e+06 output 1.443e+05 target 1.273e+06
Train Epoch: 1 [51200/81276 (63%)]	Loss: 6.220404 LR 2.97e-04
	 Magnitude of data 1.319e+06 output 1.408e+05 target 1.319e+06
Train Epoch: 1 [51840/81276 (64%)]	Loss: 6.470482 LR 2.97e-04
	 Magnitude of data 1.351e+06 output 1.496e+05 target 1.351e+06
Train Epoch: 1 [52480/81276 (65%)]	Loss: 6.202317 LR 2.97e-04
	 Magnitude of data 1.318e+06 output 1.481e+05 target 1.318e+06
Train Epoch: 1 [53120/81276 (65%)]	Loss: 6.180234 LR 2.97e-04
	 Magnitude of data 1.274e+06 output 1.423e+05 target 1.274e+06
Train Epoch: 1 [53760/81276 (66%)]	Loss: 6.464435 LR 2.97e-04
	 Magnitude of data 1.363e+06 output 1.448e+05 target 1.363e+06
Train Epoch: 1 [54400/81276 (67%)]	Loss: 6.316998 LR 2.97e-04
	 Magnitude of data 1.305e+06 output 1.508e+05 target 1.305e+06
Train Epoch: 1 [55040/81276 (68%)]	Loss: 6.353529 LR 2.97e-04
	 Magnitude of data 1.319e+06 output 1.463e+05 target 1.318e+06
Train Epoch: 1 [55680/81276 (69%)]	Loss: 6.408969 LR 2.97e-04
	 Magnitude of data 1.197e+06 output 1.489e+05 target 1.197e+06
Train Epoch: 1 [56320/81276 (69%)]	Loss: 6.334366 LR 2.96e-04
	 Magnitude of data 1.373e+06 output 1.490e+05 target 1.373e+06
Train Epoch: 1 [56960/81276 (70%)]	Loss: 6.367647 LR 2.96e-04
	 Magnitude of data 1.323e+06 output 1.497e+05 target 1.323e+06
Train Epoch: 1 [57600/81276 (71%)]	Loss: 6.222339 LR 2.96e-04
	 Magnitude of data 1.223e+06 output 1.546e+05 target 1.223e+06
Train Epoch: 1 [58240/81276 (72%)]	Loss: 6.554217 LR 2.96e-04
	 Magnitude of data 1.344e+06 output 1.489e+05 target 1.344e+06
Train Epoch: 1 [58880/81276 (72%)]	Loss: 6.469525 LR 2.96e-04
	 Magnitude of data 1.282e+06 output 1.535e+05 target 1.282e+06
Train Epoch: 1 [59520/81276 (73%)]	Loss: 6.098701 LR 2.96e-04
	 Magnitude of data 1.214e+06 output 1.524e+05 target 1.214e+06
Train Epoch: 1 [60160/81276 (74%)]	Loss: 6.305316 LR 2.96e-04
	 Magnitude of data 1.322e+06 output 1.569e+05 target 1.323e+06
Train Epoch: 1 [60800/81276 (75%)]	Loss: 6.271515 LR 2.96e-04
	 Magnitude of data 1.290e+06 output 1.578e+05 target 1.290e+06
Train Epoch: 1 [61440/81276 (76%)]	Loss: 6.221477 LR 2.96e-04
	 Magnitude of data 1.354e+06 output 1.561e+05 target 1.354e+06
Train Epoch: 1 [62080/81276 (76%)]	Loss: 6.173118 LR 2.96e-04
	 Magnitude of data 1.348e+06 output 1.540e+05 target 1.348e+06
Train Epoch: 1 [62720/81276 (77%)]	Loss: 6.257227 LR 2.96e-04
	 Magnitude of data 1.358e+06 output 1.552e+05 target 1.358e+06
Train Epoch: 1 [63360/81276 (78%)]	Loss: 6.402564 LR 2.96e-04
	 Magnitude of data 1.367e+06 output 1.530e+05 target 1.367e+06
Train Epoch: 1 [64000/81276 (79%)]	Loss: 6.206480 LR 2.95e-04
	 Magnitude of data 1.341e+06 output 1.537e+05 target 1.341e+06
Train Epoch: 1 [64640/81276 (80%)]	Loss: 6.249591 LR 2.95e-04
	 Magnitude of data 1.274e+06 output 1.593e+05 target 1.274e+06
Train Epoch: 1 [65280/81276 (80%)]	Loss: 6.268324 LR 2.95e-04
	 Magnitude of data 1.638e+06 output 1.553e+05 target 1.638e+06
Train Epoch: 1 [65920/81276 (81%)]	Loss: 6.098681 LR 2.95e-04
	 Magnitude of data 1.178e+06 output 1.581e+05 target 1.178e+06
Train Epoch: 1 [66560/81276 (82%)]	Loss: 6.393285 LR 2.95e-04
	 Magnitude of data 1.313e+06 output 1.591e+05 target 1.313e+06
Train Epoch: 1 [67200/81276 (83%)]	Loss: 6.181913 LR 2.95e-04
	 Magnitude of data 1.305e+06 output 1.587e+05 target 1.305e+06
Train Epoch: 1 [67840/81276 (84%)]	Loss: 5.960476 LR 2.95e-04
	 Magnitude of data 1.243e+06 output 1.580e+05 target 1.243e+06
Train Epoch: 1 [68480/81276 (84%)]	Loss: 6.245131 LR 2.95e-04
	 Magnitude of data 1.389e+06 output 1.601e+05 target 1.389e+06
Train Epoch: 1 [69120/81276 (85%)]	Loss: 6.225796 LR 2.95e-04
	 Magnitude of data 1.368e+06 output 1.619e+05 target 1.368e+06
Train Epoch: 1 [69760/81276 (86%)]	Loss: 6.303569 LR 2.95e-04
	 Magnitude of data 1.272e+06 output 1.696e+05 target 1.272e+06
Train Epoch: 1 [70400/81276 (87%)]	Loss: 6.333977 LR 2.94e-04
	 Magnitude of data 1.307e+06 output 1.624e+05 target 1.307e+06
Train Epoch: 1 [71040/81276 (87%)]	Loss: 6.435668 LR 2.94e-04
	 Magnitude of data 1.298e+06 output 1.618e+05 target 1.298e+06
Train Epoch: 1 [71680/81276 (88%)]	Loss: 6.224271 LR 2.94e-04
	 Magnitude of data 1.285e+06 output 1.606e+05 target 1.285e+06
Train Epoch: 1 [72320/81276 (89%)]	Loss: 6.130877 LR 2.94e-04
	 Magnitude of data 1.372e+06 output 1.620e+05 target 1.372e+06
Train Epoch: 1 [72960/81276 (90%)]	Loss: 6.190343 LR 2.94e-04
	 Magnitude of data 1.268e+06 output 1.599e+05 target 1.268e+06
Train Epoch: 1 [73600/81276 (91%)]	Loss: 6.018893 LR 2.94e-04
	 Magnitude of data 1.248e+06 output 1.657e+05 target 1.248e+06
Train Epoch: 1 [74240/81276 (91%)]	Loss: 6.285451 LR 2.94e-04
	 Magnitude of data 1.344e+06 output 1.671e+05 target 1.344e+06
Train Epoch: 1 [74880/81276 (92%)]	Loss: 6.418380 LR 2.94e-04
	 Magnitude of data 1.405e+06 output 1.642e+05 target 1.405e+06
Train Epoch: 1 [75520/81276 (93%)]	Loss: 6.128286 LR 2.94e-04
	 Magnitude of data 1.326e+06 output 1.620e+05 target 1.326e+06
Train Epoch: 1 [76160/81276 (94%)]	Loss: 6.167625 LR 2.94e-04
	 Magnitude of data 1.291e+06 output 1.646e+05 target 1.291e+06
Train Epoch: 1 [76800/81276 (95%)]	Loss: 6.222341 LR 2.93e-04
	 Magnitude of data 1.338e+06 output 1.631e+05 target 1.338e+06
Train Epoch: 1 [77440/81276 (95%)]	Loss: 6.102063 LR 2.93e-04
	 Magnitude of data 1.279e+06 output 1.683e+05 target 1.279e+06
Train Epoch: 1 [78080/81276 (96%)]	Loss: 6.082208 LR 2.93e-04
	 Magnitude of data 1.287e+06 output 1.654e+05 target 1.287e+06
Train Epoch: 1 [78720/81276 (97%)]	Loss: 6.256075 LR 2.93e-04
	 Magnitude of data 1.253e+06 output 1.633e+05 target 1.253e+06
Train Epoch: 1 [79360/81276 (98%)]	Loss: 6.387070 LR 2.93e-04
	 Magnitude of data 1.327e+06 output 1.622e+05 target 1.327e+06
Train Epoch: 1 [80000/81276 (99%)]	Loss: 6.316838 LR 2.93e-04
	 Magnitude of data 1.478e+06 output 1.640e+05 target 1.478e+06
Train Epoch: 1 [80640/81276 (99%)]	Loss: 6.149230 LR 2.93e-04
	 Magnitude of data 1.218e+06 output 1.702e+05 target 1.218e+06
Train Epoch: 1 [81216/81276 (100%)]	Loss: 5.997686 LR 2.93e-04
Test set: Average loss: 6.13217390
Train Epoch: 2 [0/81276 (0%)]	Loss: 6.235148 LR 2.93e-04
	 Magnitude of data 1.348e+06 output 1.697e+05 target 1.348e+06
Train Epoch: 2 [640/81276 (1%)]	Loss: 6.150021 LR 2.93e-04
	 Magnitude of data 1.314e+06 output 1.704e+05 target 1.314e+06
Train Epoch: 2 [1280/81276 (2%)]	Loss: 6.345514 LR 2.92e-04
	 Magnitude of data 1.495e+06 output 1.673e+05 target 1.495e+06
Train Epoch: 2 [1920/81276 (2%)]	Loss: 6.123474 LR 2.92e-04
	 Magnitude of data 1.218e+06 output 1.708e+05 target 1.218e+06
Train Epoch: 2 [2560/81276 (3%)]	Loss: 5.900722 LR 2.92e-04
	 Magnitude of data 1.281e+06 output 1.650e+05 target 1.281e+06
Train Epoch: 2 [3200/81276 (4%)]	Loss: 5.862667 LR 2.92e-04
	 Magnitude of data 1.317e+06 output 1.724e+05 target 1.317e+06
Train Epoch: 2 [3840/81276 (5%)]	Loss: 5.948021 LR 2.92e-04
	 Magnitude of data 1.281e+06 output 1.727e+05 target 1.281e+06
Train Epoch: 2 [4480/81276 (6%)]	Loss: 5.959834 LR 2.92e-04
	 Magnitude of data 1.306e+06 output 1.769e+05 target 1.306e+06
Train Epoch: 2 [5120/81276 (6%)]	Loss: 6.036442 LR 2.92e-04
	 Magnitude of data 1.360e+06 output 1.755e+05 target 1.360e+06
Train Epoch: 2 [5760/81276 (7%)]	Loss: 6.139809 LR 2.92e-04
	 Magnitude of data 1.349e+06 output 1.696e+05 target 1.350e+06
Train Epoch: 2 [6400/81276 (8%)]	Loss: 6.241316 LR 2.91e-04
	 Magnitude of data 1.285e+06 output 1.700e+05 target 1.286e+06
Train Epoch: 2 [7040/81276 (9%)]	Loss: 6.080356 LR 2.91e-04
	 Magnitude of data 1.278e+06 output 1.688e+05 target 1.278e+06
Train Epoch: 2 [7680/81276 (9%)]	Loss: 6.256611 LR 2.91e-04
	 Magnitude of data 1.419e+06 output 1.590e+05 target 1.419e+06
Train Epoch: 2 [8320/81276 (10%)]	Loss: 6.212037 LR 2.91e-04
	 Magnitude of data 1.234e+06 output 1.626e+05 target 1.234e+06
Train Epoch: 2 [8960/81276 (11%)]	Loss: 6.257266 LR 2.91e-04
	 Magnitude of data 1.348e+06 output 1.673e+05 target 1.348e+06
Train Epoch: 2 [9600/81276 (12%)]	Loss: 6.055338 LR 2.91e-04
	 Magnitude of data 1.364e+06 output 1.673e+05 target 1.364e+06
Train Epoch: 2 [10240/81276 (13%)]	Loss: 6.038257 LR 2.91e-04
	 Magnitude of data 1.276e+06 output 1.693e+05 target 1.276e+06
Train Epoch: 2 [10880/81276 (13%)]	Loss: 5.967830 LR 2.91e-04
	 Magnitude of data 1.240e+06 output 1.782e+05 target 1.240e+06
Train Epoch: 2 [11520/81276 (14%)]	Loss: 6.041111 LR 2.90e-04
	 Magnitude of data 1.342e+06 output 1.671e+05 target 1.342e+06
Train Epoch: 2 [12160/81276 (15%)]	Loss: 6.044248 LR 2.90e-04
	 Magnitude of data 1.359e+06 output 1.750e+05 target 1.359e+06
Train Epoch: 2 [12800/81276 (16%)]	Loss: 6.073336 LR 2.90e-04
	 Magnitude of data 1.366e+06 output 1.724e+05 target 1.366e+06
Train Epoch: 2 [13440/81276 (17%)]	Loss: 5.843450 LR 2.90e-04
	 Magnitude of data 1.241e+06 output 1.798e+05 target 1.241e+06
Train Epoch: 2 [14080/81276 (17%)]	Loss: 6.123330 LR 2.90e-04
	 Magnitude of data 1.396e+06 output 1.747e+05 target 1.396e+06
Train Epoch: 2 [14720/81276 (18%)]	Loss: 6.237317 LR 2.90e-04
	 Magnitude of data 1.450e+06 output 1.739e+05 target 1.450e+06
Train Epoch: 2 [15360/81276 (19%)]	Loss: 6.277137 LR 2.90e-04
	 Magnitude of data 1.387e+06 output 1.747e+05 target 1.387e+06
Train Epoch: 2 [16000/81276 (20%)]	Loss: 6.045535 LR 2.90e-04
	 Magnitude of data 1.371e+06 output 1.728e+05 target 1.371e+06
Train Epoch: 2 [16640/81276 (20%)]	Loss: 5.907603 LR 2.89e-04
	 Magnitude of data 1.274e+06 output 1.782e+05 target 1.274e+06
Train Epoch: 2 [17280/81276 (21%)]	Loss: 6.271310 LR 2.89e-04
	 Magnitude of data 1.443e+06 output 1.744e+05 target 1.442e+06
Train Epoch: 2 [17920/81276 (22%)]	Loss: 6.051390 LR 2.89e-04
	 Magnitude of data 1.267e+06 output 1.729e+05 target 1.266e+06
Train Epoch: 2 [18560/81276 (23%)]	Loss: 5.899560 LR 2.89e-04
	 Magnitude of data 1.246e+06 output 1.804e+05 target 1.246e+06
Train Epoch: 2 [19200/81276 (24%)]	Loss: 5.907565 LR 2.89e-04
	 Magnitude of data 1.254e+06 output 1.825e+05 target 1.254e+06
Train Epoch: 2 [19840/81276 (24%)]	Loss: 6.011249 LR 2.89e-04
	 Magnitude of data 1.331e+06 output 1.811e+05 target 1.331e+06
Train Epoch: 2 [20480/81276 (25%)]	Loss: 5.965196 LR 2.89e-04
	 Magnitude of data 1.190e+06 output 1.772e+05 target 1.190e+06
Train Epoch: 2 [21120/81276 (26%)]	Loss: 6.052470 LR 2.88e-04
	 Magnitude of data 1.290e+06 output 1.686e+05 target 1.290e+06
Train Epoch: 2 [21760/81276 (27%)]	Loss: 6.009603 LR 2.88e-04
	 Magnitude of data 1.386e+06 output 1.745e+05 target 1.385e+06
Train Epoch: 2 [22400/81276 (28%)]	Loss: 5.845360 LR 2.88e-04
	 Magnitude of data 1.271e+06 output 1.767e+05 target 1.271e+06
Train Epoch: 2 [23040/81276 (28%)]	Loss: 6.200585 LR 2.88e-04
	 Magnitude of data 1.259e+06 output 1.684e+05 target 1.259e+06
Train Epoch: 2 [23680/81276 (29%)]	Loss: 5.924232 LR 2.88e-04
	 Magnitude of data 1.231e+06 output 1.815e+05 target 1.231e+06
Train Epoch: 2 [24320/81276 (30%)]	Loss: 6.107809 LR 2.88e-04
	 Magnitude of data 1.323e+06 output 1.773e+05 target 1.322e+06
Train Epoch: 2 [24960/81276 (31%)]	Loss: 5.772715 LR 2.88e-04
	 Magnitude of data 1.341e+06 output 1.755e+05 target 1.341e+06
Train Epoch: 2 [25600/81276 (32%)]	Loss: 5.799312 LR 2.87e-04
	 Magnitude of data 1.299e+06 output 1.771e+05 target 1.299e+06
Train Epoch: 2 [26240/81276 (32%)]	Loss: 6.089917 LR 2.87e-04
	 Magnitude of data 1.329e+06 output 1.796e+05 target 1.329e+06
Train Epoch: 2 [26880/81276 (33%)]	Loss: 5.656352 LR 2.87e-04
	 Magnitude of data 1.258e+06 output 1.800e+05 target 1.258e+06
Train Epoch: 2 [27520/81276 (34%)]	Loss: 5.993775 LR 2.87e-04
	 Magnitude of data 1.280e+06 output 1.792e+05 target 1.280e+06
Train Epoch: 2 [28160/81276 (35%)]	Loss: 5.952795 LR 2.87e-04
	 Magnitude of data 1.353e+06 output 1.758e+05 target 1.353e+06
Train Epoch: 2 [28800/81276 (35%)]	Loss: 5.620257 LR 2.87e-04
	 Magnitude of data 1.316e+06 output 1.819e+05 target 1.316e+06
Train Epoch: 2 [29440/81276 (36%)]	Loss: 5.968818 LR 2.86e-04
	 Magnitude of data 1.295e+06 output 1.725e+05 target 1.295e+06
Train Epoch: 2 [30080/81276 (37%)]	Loss: 5.853872 LR 2.86e-04
	 Magnitude of data 1.290e+06 output 1.786e+05 target 1.290e+06
Train Epoch: 2 [30720/81276 (38%)]	Loss: 5.770079 LR 2.86e-04
	 Magnitude of data 1.320e+06 output 1.734e+05 target 1.320e+06
Train Epoch: 2 [31360/81276 (39%)]	Loss: 5.858143 LR 2.86e-04
	 Magnitude of data 1.397e+06 output 1.756e+05 target 1.397e+06
Train Epoch: 2 [32000/81276 (39%)]	Loss: 5.918543 LR 2.86e-04
	 Magnitude of data 1.283e+06 output 1.787e+05 target 1.283e+06
Train Epoch: 2 [32640/81276 (40%)]	Loss: 6.210092 LR 2.86e-04
	 Magnitude of data 1.354e+06 output 1.732e+05 target 1.354e+06
Train Epoch: 2 [33280/81276 (41%)]	Loss: 5.999587 LR 2.86e-04
	 Magnitude of data 1.271e+06 output 1.797e+05 target 1.271e+06
Train Epoch: 2 [33920/81276 (42%)]	Loss: 5.949173 LR 2.85e-04
	 Magnitude of data 1.296e+06 output 1.841e+05 target 1.296e+06
Train Epoch: 2 [34560/81276 (43%)]	Loss: 6.017359 LR 2.85e-04
	 Magnitude of data 1.312e+06 output 1.796e+05 target 1.312e+06
Train Epoch: 2 [35200/81276 (43%)]	Loss: 6.362232 LR 2.85e-04
	 Magnitude of data 1.383e+06 output 1.742e+05 target 1.383e+06
Train Epoch: 2 [35840/81276 (44%)]	Loss: 5.916032 LR 2.85e-04
	 Magnitude of data 1.314e+06 output 1.790e+05 target 1.314e+06
Train Epoch: 2 [36480/81276 (45%)]	Loss: 5.990607 LR 2.85e-04
	 Magnitude of data 1.359e+06 output 1.757e+05 target 1.359e+06
Train Epoch: 2 [37120/81276 (46%)]	Loss: 5.509278 LR 2.85e-04
	 Magnitude of data 1.273e+06 output 1.789e+05 target 1.273e+06
Train Epoch: 2 [37760/81276 (46%)]	Loss: 5.716280 LR 2.84e-04
	 Magnitude of data 1.261e+06 output 1.778e+05 target 1.261e+06
Train Epoch: 2 [38400/81276 (47%)]	Loss: 5.593684 LR 2.84e-04
	 Magnitude of data 1.358e+06 output 1.826e+05 target 1.358e+06
Train Epoch: 2 [39040/81276 (48%)]	Loss: 5.899079 LR 2.84e-04
	 Magnitude of data 1.339e+06 output 1.826e+05 target 1.339e+06
Train Epoch: 2 [39680/81276 (49%)]	Loss: 5.671482 LR 2.84e-04
	 Magnitude of data 1.233e+06 output 1.874e+05 target 1.233e+06
Train Epoch: 2 [40320/81276 (50%)]	Loss: 5.850895 LR 2.84e-04
	 Magnitude of data 1.395e+06 output 1.781e+05 target 1.395e+06
Train Epoch: 2 [40960/81276 (50%)]	Loss: 5.759170 LR 2.84e-04
	 Magnitude of data 1.324e+06 output 1.785e+05 target 1.324e+06
Train Epoch: 2 [41600/81276 (51%)]	Loss: 5.833452 LR 2.83e-04
	 Magnitude of data 1.367e+06 output 1.694e+05 target 1.367e+06
Train Epoch: 2 [42240/81276 (52%)]	Loss: 5.841092 LR 2.83e-04
	 Magnitude of data 1.301e+06 output 1.801e+05 target 1.301e+06
Train Epoch: 2 [42880/81276 (53%)]	Loss: 5.509534 LR 2.83e-04
	 Magnitude of data 1.223e+06 output 1.780e+05 target 1.223e+06
Train Epoch: 2 [43520/81276 (54%)]	Loss: 5.809522 LR 2.83e-04
	 Magnitude of data 1.285e+06 output 1.679e+05 target 1.285e+06
Train Epoch: 2 [44160/81276 (54%)]	Loss: 5.736009 LR 2.83e-04
	 Magnitude of data 1.281e+06 output 1.814e+05 target 1.281e+06
Train Epoch: 2 [44800/81276 (55%)]	Loss: 5.716463 LR 2.83e-04
	 Magnitude of data 1.413e+06 output 1.781e+05 target 1.413e+06
Train Epoch: 2 [45440/81276 (56%)]	Loss: 5.870461 LR 2.82e-04
	 Magnitude of data 1.310e+06 output 1.808e+05 target 1.310e+06
Train Epoch: 2 [46080/81276 (57%)]	Loss: 5.964475 LR 2.82e-04
	 Magnitude of data 1.308e+06 output 1.785e+05 target 1.308e+06
Train Epoch: 2 [46720/81276 (58%)]	Loss: 5.948159 LR 2.82e-04
	 Magnitude of data 1.370e+06 output 1.837e+05 target 1.371e+06
Train Epoch: 2 [47360/81276 (58%)]	Loss: 5.608438 LR 2.82e-04
	 Magnitude of data 1.361e+06 output 1.790e+05 target 1.361e+06
Train Epoch: 2 [48000/81276 (59%)]	Loss: 5.931684 LR 2.82e-04
	 Magnitude of data 1.285e+06 output 1.767e+05 target 1.286e+06
Train Epoch: 2 [48640/81276 (60%)]	Loss: 5.596269 LR 2.81e-04
	 Magnitude of data 1.249e+06 output 1.803e+05 target 1.249e+06
Train Epoch: 2 [49280/81276 (61%)]	Loss: 5.698016 LR 2.81e-04
	 Magnitude of data 1.305e+06 output 1.780e+05 target 1.305e+06
Train Epoch: 2 [49920/81276 (61%)]	Loss: 5.706699 LR 2.81e-04
	 Magnitude of data 1.259e+06 output 1.737e+05 target 1.259e+06
Train Epoch: 2 [50560/81276 (62%)]	Loss: 5.798713 LR 2.81e-04
	 Magnitude of data 1.274e+06 output 1.772e+05 target 1.273e+06
Train Epoch: 2 [51200/81276 (63%)]	Loss: 5.560857 LR 2.81e-04
	 Magnitude of data 1.319e+06 output 1.869e+05 target 1.319e+06
Train Epoch: 2 [51840/81276 (64%)]	Loss: 5.803271 LR 2.81e-04
	 Magnitude of data 1.351e+06 output 1.839e+05 target 1.351e+06
Train Epoch: 2 [52480/81276 (65%)]	Loss: 5.524479 LR 2.80e-04
	 Magnitude of data 1.318e+06 output 1.823e+05 target 1.318e+06
Train Epoch: 2 [53120/81276 (65%)]	Loss: 5.535365 LR 2.80e-04
	 Magnitude of data 1.274e+06 output 1.825e+05 target 1.274e+06
Train Epoch: 2 [53760/81276 (66%)]	Loss: 5.834453 LR 2.80e-04
	 Magnitude of data 1.363e+06 output 1.778e+05 target 1.363e+06
Train Epoch: 2 [54400/81276 (67%)]	Loss: 5.696514 LR 2.80e-04
	 Magnitude of data 1.305e+06 output 1.822e+05 target 1.305e+06
Train Epoch: 2 [55040/81276 (68%)]	Loss: 5.722844 LR 2.80e-04
	 Magnitude of data 1.319e+06 output 1.858e+05 target 1.318e+06
Train Epoch: 2 [55680/81276 (69%)]	Loss: 5.859785 LR 2.79e-04
	 Magnitude of data 1.197e+06 output 1.832e+05 target 1.197e+06
Train Epoch: 2 [56320/81276 (69%)]	Loss: 5.676372 LR 2.79e-04
	 Magnitude of data 1.373e+06 output 1.787e+05 target 1.373e+06
Train Epoch: 2 [56960/81276 (70%)]	Loss: 5.749922 LR 2.79e-04
	 Magnitude of data 1.323e+06 output 1.742e+05 target 1.323e+06
Train Epoch: 2 [57600/81276 (71%)]	Loss: 5.605000 LR 2.79e-04
	 Magnitude of data 1.223e+06 output 1.840e+05 target 1.223e+06
Train Epoch: 2 [58240/81276 (72%)]	Loss: 5.938491 LR 2.79e-04
	 Magnitude of data 1.344e+06 output 1.714e+05 target 1.344e+06
Train Epoch: 2 [58880/81276 (72%)]	Loss: 5.860308 LR 2.78e-04
	 Magnitude of data 1.282e+06 output 1.730e+05 target 1.282e+06
Train Epoch: 2 [59520/81276 (73%)]	Loss: 5.518003 LR 2.78e-04
	 Magnitude of data 1.214e+06 output 1.846e+05 target 1.214e+06
Train Epoch: 2 [60160/81276 (74%)]	Loss: 5.760840 LR 2.78e-04
	 Magnitude of data 1.322e+06 output 1.853e+05 target 1.323e+06
Train Epoch: 2 [60800/81276 (75%)]	Loss: 5.701477 LR 2.78e-04
	 Magnitude of data 1.290e+06 output 1.836e+05 target 1.290e+06
Train Epoch: 2 [61440/81276 (76%)]	Loss: 5.564334 LR 2.78e-04
	 Magnitude of data 1.354e+06 output 1.776e+05 target 1.354e+06
Train Epoch: 2 [62080/81276 (76%)]	Loss: 5.536515 LR 2.78e-04
	 Magnitude of data 1.348e+06 output 1.847e+05 target 1.348e+06
Train Epoch: 2 [62720/81276 (77%)]	Loss: 5.693657 LR 2.77e-04
	 Magnitude of data 1.358e+06 output 1.798e+05 target 1.358e+06
Train Epoch: 2 [63360/81276 (78%)]	Loss: 5.826401 LR 2.77e-04
	 Magnitude of data 1.367e+06 output 1.742e+05 target 1.367e+06
Train Epoch: 2 [64000/81276 (79%)]	Loss: 5.516401 LR 2.77e-04
	 Magnitude of data 1.341e+06 output 1.754e+05 target 1.341e+06
Train Epoch: 2 [64640/81276 (80%)]	Loss: 5.789512 LR 2.77e-04
	 Magnitude of data 1.274e+06 output 1.798e+05 target 1.274e+06
Train Epoch: 2 [65280/81276 (80%)]	Loss: 5.512261 LR 2.77e-04
	 Magnitude of data 1.638e+06 output 1.819e+05 target 1.638e+06
Train Epoch: 2 [65920/81276 (81%)]	Loss: 5.528385 LR 2.76e-04
	 Magnitude of data 1.178e+06 output 1.836e+05 target 1.178e+06
Train Epoch: 2 [66560/81276 (82%)]	Loss: 5.836186 LR 2.76e-04
	 Magnitude of data 1.313e+06 output 1.771e+05 target 1.313e+06
Train Epoch: 2 [67200/81276 (83%)]	Loss: 5.648158 LR 2.76e-04
	 Magnitude of data 1.305e+06 output 1.822e+05 target 1.305e+06
Train Epoch: 2 [67840/81276 (84%)]	Loss: 5.418675 LR 2.76e-04
	 Magnitude of data 1.243e+06 output 1.778e+05 target 1.243e+06
Train Epoch: 2 [68480/81276 (84%)]	Loss: 5.646799 LR 2.76e-04
	 Magnitude of data 1.389e+06 output 1.809e+05 target 1.389e+06
Train Epoch: 2 [69120/81276 (85%)]	Loss: 5.697066 LR 2.75e-04
	 Magnitude of data 1.368e+06 output 1.824e+05 target 1.368e+06
Train Epoch: 2 [69760/81276 (86%)]	Loss: 5.803187 LR 2.75e-04
	 Magnitude of data 1.272e+06 output 1.825e+05 target 1.272e+06
Train Epoch: 2 [70400/81276 (87%)]	Loss: 5.827614 LR 2.75e-04
	 Magnitude of data 1.307e+06 output 1.784e+05 target 1.307e+06
Train Epoch: 2 [71040/81276 (87%)]	Loss: 5.906230 LR 2.75e-04
	 Magnitude of data 1.298e+06 output 1.740e+05 target 1.298e+06
Train Epoch: 2 [71680/81276 (88%)]	Loss: 5.705412 LR 2.75e-04
	 Magnitude of data 1.285e+06 output 1.810e+05 target 1.285e+06
Train Epoch: 2 [72320/81276 (89%)]	Loss: 5.557763 LR 2.74e-04
	 Magnitude of data 1.372e+06 output 1.792e+05 target 1.372e+06
Train Epoch: 2 [72960/81276 (90%)]	Loss: 5.694922 LR 2.74e-04
	 Magnitude of data 1.268e+06 output 1.773e+05 target 1.268e+06
Train Epoch: 2 [73600/81276 (91%)]	Loss: 5.498083 LR 2.74e-04
	 Magnitude of data 1.248e+06 output 1.812e+05 target 1.248e+06
Train Epoch: 2 [74240/81276 (91%)]	Loss: 5.762490 LR 2.74e-04
	 Magnitude of data 1.344e+06 output 1.791e+05 target 1.344e+06
Train Epoch: 2 [74880/81276 (92%)]	Loss: 5.915797 LR 2.73e-04
	 Magnitude of data 1.405e+06 output 1.780e+05 target 1.405e+06
Train Epoch: 2 [75520/81276 (93%)]	Loss: 5.612777 LR 2.73e-04
	 Magnitude of data 1.326e+06 output 1.756e+05 target 1.326e+06
Train Epoch: 2 [76160/81276 (94%)]	Loss: 5.628134 LR 2.73e-04
	 Magnitude of data 1.291e+06 output 1.756e+05 target 1.291e+06
Train Epoch: 2 [76800/81276 (95%)]	Loss: 5.716693 LR 2.73e-04
	 Magnitude of data 1.338e+06 output 1.739e+05 target 1.338e+06
Train Epoch: 2 [77440/81276 (95%)]	Loss: 5.570191 LR 2.73e-04
	 Magnitude of data 1.279e+06 output 1.820e+05 target 1.279e+06
Train Epoch: 2 [78080/81276 (96%)]	Loss: 5.544549 LR 2.72e-04
	 Magnitude of data 1.287e+06 output 1.796e+05 target 1.287e+06
Train Epoch: 2 [78720/81276 (97%)]	Loss: 5.681669 LR 2.72e-04
	 Magnitude of data 1.253e+06 output 1.833e+05 target 1.253e+06
Train Epoch: 2 [79360/81276 (98%)]	Loss: 5.920984 LR 2.72e-04
	 Magnitude of data 1.327e+06 output 1.780e+05 target 1.327e+06
Train Epoch: 2 [80000/81276 (99%)]	Loss: 5.738280 LR 2.72e-04
	 Magnitude of data 1.478e+06 output 1.773e+05 target 1.478e+06
Train Epoch: 2 [80640/81276 (99%)]	Loss: 5.660641 LR 2.72e-04
	 Magnitude of data 1.218e+06 output 1.796e+05 target 1.218e+06
Train Epoch: 2 [81216/81276 (100%)]	Loss: 5.441617 LR 2.71e-04
Test set: Average loss: 5.67425770
Train Epoch: 3 [0/81276 (0%)]	Loss: 5.750619 LR 2.71e-04
	 Magnitude of data 1.348e+06 output 1.842e+05 target 1.348e+06
Train Epoch: 3 [640/81276 (1%)]	Loss: 5.653826 LR 2.71e-04
	 Magnitude of data 1.314e+06 output 1.845e+05 target 1.314e+06
Train Epoch: 3 [1280/81276 (2%)]	Loss: 5.883900 LR 2.71e-04
	 Magnitude of data 1.495e+06 output 1.783e+05 target 1.495e+06
Train Epoch: 3 [1920/81276 (2%)]	Loss: 5.659265 LR 2.71e-04
	 Magnitude of data 1.218e+06 output 1.860e+05 target 1.218e+06
Train Epoch: 3 [2560/81276 (3%)]	Loss: 5.376590 LR 2.70e-04
	 Magnitude of data 1.281e+06 output 1.766e+05 target 1.281e+06
Train Epoch: 3 [3200/81276 (4%)]	Loss: 5.342812 LR 2.70e-04
	 Magnitude of data 1.317e+06 output 1.821e+05 target 1.317e+06
Train Epoch: 3 [3840/81276 (5%)]	Loss: 5.433020 LR 2.70e-04
	 Magnitude of data 1.281e+06 output 1.868e+05 target 1.281e+06
Train Epoch: 3 [4480/81276 (6%)]	Loss: 5.459920 LR 2.70e-04
	 Magnitude of data 1.306e+06 output 1.868e+05 target 1.306e+06
Train Epoch: 3 [5120/81276 (6%)]	Loss: 5.517859 LR 2.70e-04
	 Magnitude of data 1.360e+06 output 1.843e+05 target 1.360e+06
Train Epoch: 3 [5760/81276 (7%)]	Loss: 5.699232 LR 2.69e-04
	 Magnitude of data 1.349e+06 output 1.828e+05 target 1.350e+06
Train Epoch: 3 [6400/81276 (8%)]	Loss: 5.720833 LR 2.69e-04
	 Magnitude of data 1.285e+06 output 1.817e+05 target 1.286e+06
Train Epoch: 3 [7040/81276 (9%)]	Loss: 5.597010 LR 2.69e-04
	 Magnitude of data 1.278e+06 output 1.806e+05 target 1.278e+06
Train Epoch: 3 [7680/81276 (9%)]	Loss: 5.737885 LR 2.69e-04
	 Magnitude of data 1.419e+06 output 1.748e+05 target 1.419e+06
Train Epoch: 3 [8320/81276 (10%)]	Loss: 5.763116 LR 2.68e-04
	 Magnitude of data 1.234e+06 output 1.689e+05 target 1.234e+06
Train Epoch: 3 [8960/81276 (11%)]	Loss: 5.815763 LR 2.68e-04
	 Magnitude of data 1.348e+06 output 1.788e+05 target 1.348e+06
Train Epoch: 3 [9600/81276 (12%)]	Loss: 5.606016 LR 2.68e-04
	 Magnitude of data 1.364e+06 output 1.755e+05 target 1.364e+06
Train Epoch: 3 [10240/81276 (13%)]	Loss: 5.576602 LR 2.68e-04
	 Magnitude of data 1.276e+06 output 1.779e+05 target 1.276e+06
Train Epoch: 3 [10880/81276 (13%)]	Loss: 5.467636 LR 2.68e-04
	 Magnitude of data 1.240e+06 output 1.869e+05 target 1.240e+06
Train Epoch: 3 [11520/81276 (14%)]	Loss: 5.562201 LR 2.67e-04
	 Magnitude of data 1.342e+06 output 1.781e+05 target 1.342e+06
Train Epoch: 3 [12160/81276 (15%)]	Loss: 5.522604 LR 2.67e-04
	 Magnitude of data 1.359e+06 output 1.848e+05 target 1.359e+06
Train Epoch: 3 [12800/81276 (16%)]	Loss: 5.618689 LR 2.67e-04
	 Magnitude of data 1.366e+06 output 1.854e+05 target 1.366e+06
Train Epoch: 3 [13440/81276 (17%)]	Loss: 5.374402 LR 2.67e-04
	 Magnitude of data 1.241e+06 output 1.864e+05 target 1.241e+06
Train Epoch: 3 [14080/81276 (17%)]	Loss: 5.701488 LR 2.66e-04
	 Magnitude of data 1.396e+06 output 1.856e+05 target 1.396e+06
Train Epoch: 3 [14720/81276 (18%)]	Loss: 5.794847 LR 2.66e-04
	 Magnitude of data 1.450e+06 output 1.829e+05 target 1.450e+06
