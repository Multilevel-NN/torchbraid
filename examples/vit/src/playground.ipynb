{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1eef00-4bec-42d4-ad49-061ffe5ddad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7216dd6685d940c4bca2342f211ef493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-huge-patch14-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1280,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5120,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"patch_size\": 14,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.48.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTConfig\n",
    "\n",
    "# Load the configuration for the ViT Huge model\n",
    "config = ViTConfig.from_pretrained(\"google/vit-huge-patch14-224-in21k\")\n",
    "\n",
    "# Print the configuration to inspect its attributes\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223f9114-2bbe-4f3c-a396-8f13f6349035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eager'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config._attn_implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a58f1-5673-4cce-b891-297ce9585156",
   "metadata": {},
   "source": [
    "# My alteration of the implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ba303b-34c1-40bd-afd8-3d685320383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# My modifications of the ViT from Huggingface. Huge simplification \n",
    "# ---------------------------------------------------\n",
    "\"\"\"PyTorch ViT model.\"\"\"\n",
    "\n",
    "import collections.abc\n",
    "import math\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPooling,\n",
    "    ImageClassifierOutput,\n",
    ")\n",
    "from transformers import ViTPreTrainedModel\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bd09909e-e959-4790-99aa-798e9b30b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config class for ViT \n",
    "class ViTConfig(PretrainedConfig):\n",
    "    model_type = \"vit\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "        initializer_range=0.02,\n",
    "        layer_norm_eps=1e-12,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        qkv_bias=True,\n",
    "        encoder_stride=16,\n",
    "        pooler_output_size=None,\n",
    "        pooler_act=\"tanh\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_act = hidden_act\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.initializer_range = initializer_range\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.encoder_stride = encoder_stride\n",
    "        self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n",
    "        self.pooler_act = pooler_act\n",
    "\n",
    "\n",
    "def torch_int(x):\n",
    "    \"\"\"\n",
    "    Casts an input to a torch int64 tensor if we are in a tracing context, otherwise to a Python int.\n",
    "    \"\"\"\n",
    "    return x.to(torch.int64) if torch.jit.is_tracing() and isinstance(x, torch.Tensor) else int(x)\n",
    "\n",
    "\n",
    "class ViTEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "        self.patch_embeddings = ViTPatchEmbeddings(config)\n",
    "        num_patches = self.patch_embeddings.num_patches\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.patch_size = config.patch_size\n",
    "        self.config = config\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.patch_embeddings(pixel_values)\n",
    "\n",
    "        # add the [CLS] token to the embedded patch tokens\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        embeddings = embeddings + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class ViTPatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n",
    "    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n",
    "    Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        image_size, patch_size = config.image_size, config.patch_size\n",
    "        num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_channels, height, width = pixel_values.shape\n",
    "        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n",
    "\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "\n",
    "    attn_output = torch.matmul(attn_weights, value)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class ViTSelfAttention(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n",
    "                f\"heads {config.num_attention_heads}.\"\n",
    "            )\n",
    "\n",
    "        self.config = config\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.dropout_prob = config.attention_probs_dropout_prob\n",
    "        self.scaling = self.attention_head_size**-0.5\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        context_layer, attention_probs = attention_interface(\n",
    "            self,\n",
    "            query_layer,\n",
    "            key_layer,\n",
    "            value_layer,\n",
    "            scaling=self.scaling,\n",
    "            dropout=0.0 if not self.training else self.dropout_prob,\n",
    "        )\n",
    "\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.reshape(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ViTSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
    "    layernorm applied before each block.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ViTAttention(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTSelfAttention(config)\n",
    "        self.output = ViTSelfOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        self_outputs = self.attention(hidden_states)\n",
    "\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "\n",
    "        # outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class ViTIntermediate(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ViTOutput(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        hidden_states = hidden_states + input_tensor\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class ViTLayer(nn.Module):\n",
    "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
    "\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = ViTAttention(config)\n",
    "        self.intermediate = ViTIntermediate(config)\n",
    "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "        # ViTOutput parameters\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        dt: float = 1\n",
    "    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "        self_attention_outputs = self.attention(\n",
    "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention (pre-layer norm)\n",
    "        )\n",
    "        attention_output = self_attention_outputs\n",
    "\n",
    "        # first residual connection\n",
    "        # corresponds to x_{t + 1/2} \n",
    "        hidden_states = dt * attention_output + hidden_states\n",
    "\n",
    "        # in ViT, layernorm is also applied after self-attention\n",
    "        mlp_output = self.dropout(\n",
    "            self.dense(self.intermediate(self.layernorm_after(hidden_states)))\n",
    "        )\n",
    "\n",
    "        # second residual connection is done here corresponds to x_{t + 1}\n",
    "        layer_output =  dt * mlp_output + hidden_states\n",
    "\n",
    "        return layer_output\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "    ) -> Union[tuple, BaseModelOutput]:\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            layer_outputs = layer_module(hidden_states)\n",
    "            hidden_states = layer_outputs\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class ViTModel(ViTPreTrainedModel):\n",
    "    def __init__(self, config: ViTConfig, add_pooling_layer: bool = True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = ViTEmbeddings(config)\n",
    "        self.encoder = ViTEncoder(config)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self) -> ViTPatchEmbeddings:\n",
    "        return self.embeddings.patch_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n",
    "            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n",
    "        \"\"\"\n",
    "        output_attentions = self.config.output_attentions\n",
    "        output_hidden_states = self.config.output_hidden_states\n",
    "        return_dict = self.config.use_return_dict\n",
    "\n",
    "        expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype\n",
    "        if pixel_values.dtype != expected_dtype:\n",
    "            print('PLEASE CONVERT DTYPE EARLIER')\n",
    "            pixel_values = pixel_values.to(expected_dtype)\n",
    "\n",
    "        # This corresponds to mapping (batch_size, num_channels, image_size, image_size)\n",
    "        # to (batch_size, (image_size / patch_size) ** 2 + 1, hidden_dim)\n",
    "        embedding_output = self.embeddings(\n",
    "            pixel_values\n",
    "        )\n",
    "\n",
    "        # ------------ TRANSFORMER -----------\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "        )\n",
    "        # ------------------------------------\n",
    "\n",
    "        sequence_output = encoder_outputs\n",
    "        sequence_output = self.layernorm(sequence_output)\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "        )\n",
    "\n",
    "\n",
    "class ViTPooler(nn.Module):\n",
    "    def __init__(self, config: ViTConfig):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.pooler_output_size)\n",
    "        self.activation = ACT2FN[config.pooler_act]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class ViTForImageClassification(ViTPreTrainedModel):\n",
    "    def __init__(self, config: ViTConfig) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ) -> Union[tuple, ImageClassifierOutput]:\n",
    "        return_dict = self.config.use_return_dict\n",
    "\n",
    "        outputs = self.vit(\n",
    "            pixel_values,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.classifier(sequence_output[:, 0, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58187-f63a-4e73-b9e2-24c042c42da6",
   "metadata": {},
   "source": [
    "## Generate some fake data and pass it through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2708094e-2afb-4782-80d6-fa5373923101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration values\n",
    "config = ViTConfig()\n",
    "batch_size = 16  # You can change this to any batch size for debugging\n",
    "num_channels = config.num_channels\n",
    "image_size = config.image_size\n",
    "\n",
    "# Generate a random tensor to simulate input images\n",
    "pixel_values = torch.randn(batch_size, num_channels, image_size, image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a47acc4-80c1-421a-a9c8-e37fe8da829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(image_size // config.patch_size) ** 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "683d13f4-7433-411a-9ad3-89872a34550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f688c198-9527-48a6-b6d4-c9a865286feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-0.2023, -0.7164],\n",
       "        [-0.2489, -0.7501],\n",
       "        [-0.2275, -0.7045],\n",
       "        [-0.2371, -0.7482],\n",
       "        [-0.2382, -0.7434],\n",
       "        [-0.2459, -0.7129],\n",
       "        [-0.2349, -0.7487],\n",
       "        [-0.2512, -0.7378],\n",
       "        [-0.2355, -0.7442],\n",
       "        [-0.2307, -0.7017],\n",
       "        [-0.2399, -0.7160],\n",
       "        [-0.2525, -0.7432],\n",
       "        [-0.2307, -0.7676],\n",
       "        [-0.2191, -0.7354],\n",
       "        [-0.2283, -0.7469],\n",
       "        [-0.2260, -0.7171]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViTForImageClassification(config)\n",
    "vit(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc0c51d8-0f49-4efd-bca3-81c3905e9e4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-1.4650,  0.6309, -0.7685,  ..., -0.5198, -0.1171,  0.8390],\n",
       "         [-1.1127, -1.4853, -0.9840,  ...,  0.9636, -1.5880,  0.8416],\n",
       "         [-0.6881, -0.7631,  0.0468,  ...,  1.0159, -2.0294, -1.0521],\n",
       "         ...,\n",
       "         [-2.7608,  0.9860, -0.9313,  ..., -0.4179,  1.4938,  2.0327],\n",
       "         [ 1.6962, -0.3185,  1.1012,  ...,  1.5161,  1.1712, -1.0406],\n",
       "         [-1.8601,  1.1686,  0.9730,  ..., -0.4404, -0.0511,  0.2571]],\n",
       "\n",
       "        [[-1.4312,  0.6125, -0.7738,  ..., -0.5061, -0.1655,  0.8885],\n",
       "         [-0.8113, -1.1424, -0.3182,  ...,  0.1327, -1.5619,  1.4042],\n",
       "         [-0.5878, -0.2349, -0.5828,  ...,  0.9349, -1.5772, -0.7292],\n",
       "         ...,\n",
       "         [-2.7279,  1.1514, -0.9776,  ..., -0.3219,  0.9072,  2.5402],\n",
       "         [ 0.9307,  0.0343,  0.5141,  ...,  0.7472,  1.7434, -0.8932],\n",
       "         [-1.6496, -0.4214,  0.2804,  ..., -0.8879, -0.0940, -1.1667]],\n",
       "\n",
       "        [[-1.4040,  0.6520, -0.8245,  ..., -0.4521, -0.1125,  0.8932],\n",
       "         [-0.2886, -0.7718, -0.7842,  ..., -0.4859, -2.1702,  1.0864],\n",
       "         [-0.9928, -0.6125, -0.8746,  ...,  0.4830, -1.9716,  0.2726],\n",
       "         ...,\n",
       "         [-1.8188,  1.4443, -0.3557,  ..., -0.0586,  1.1354,  2.4047],\n",
       "         [ 0.8810,  0.8547,  0.8806,  ...,  0.9312,  1.2601, -1.1159],\n",
       "         [-2.5824, -1.1241,  0.7814,  ..., -0.7595, -0.6129, -0.6411]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3919,  0.5575, -0.8313,  ..., -0.4356, -0.1304,  0.8111],\n",
       "         [ 0.5476, -1.5594, -1.6679,  ...,  0.6862, -0.9459,  0.7291],\n",
       "         [-0.1631, -0.7199, -1.5397,  ...,  0.4462, -0.6985, -0.8401],\n",
       "         ...,\n",
       "         [-2.7099,  1.0650, -0.2508,  ..., -0.9179,  1.9866,  3.2410],\n",
       "         [ 0.3979,  0.5757,  0.2025,  ...,  1.1107,  1.5296, -0.6752],\n",
       "         [-2.1342, -0.2697,  1.0983,  ..., -0.9758, -0.7048, -0.2789]],\n",
       "\n",
       "        [[-1.4439,  0.6544, -0.7908,  ..., -0.4763, -0.1571,  0.8958],\n",
       "         [-0.8807, -2.3568, -0.4069,  ...,  0.2364, -1.1873,  0.4270],\n",
       "         [-0.6843,  1.2098,  0.2620,  ...,  0.4682, -1.5642, -1.1487],\n",
       "         ...,\n",
       "         [-2.4538,  0.7068, -1.3073,  ..., -0.3732,  0.6057,  2.6442],\n",
       "         [ 1.9794,  0.5623,  0.3282,  ...,  0.2609,  2.2486, -0.3505],\n",
       "         [-2.0115,  0.2066,  1.0955,  ...,  0.1774, -1.2107,  0.0376]],\n",
       "\n",
       "        [[-1.4496,  0.6202, -0.7903,  ..., -0.5160, -0.0880,  0.8595],\n",
       "         [-0.7780, -1.1578, -1.7280,  ..., -0.8848, -1.6417,  1.0248],\n",
       "         [-0.6523, -0.2899, -0.1870,  ..., -0.3169, -1.1368, -1.8599],\n",
       "         ...,\n",
       "         [-2.3348,  0.9059, -1.3950,  ...,  0.4673,  2.0740,  2.4448],\n",
       "         [ 0.8113,  0.3222,  1.1213,  ...,  0.1881,  2.0165, -1.1446],\n",
       "         [-2.3243, -0.2792,  0.5440,  ..., -0.0481,  0.8089,  0.1893]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.1932, -0.3909, -0.0779,  ...,  0.7889,  0.8891, -0.4769],\n",
       "        [-0.1903, -0.3968, -0.0774,  ...,  0.7849,  0.8884, -0.4795],\n",
       "        [-0.1829, -0.3903, -0.0731,  ...,  0.7927,  0.8841, -0.4820],\n",
       "        ...,\n",
       "        [-0.1659, -0.3739, -0.0742,  ...,  0.7910,  0.8912, -0.4936],\n",
       "        [-0.1658, -0.3969, -0.0739,  ...,  0.7876,  0.8930, -0.4698],\n",
       "        [-0.1879, -0.3935, -0.0753,  ...,  0.7930,  0.8895, -0.4558]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = ViTModel(config)\n",
    "vit(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31479303-55db-487e-8c9b-bf8234490755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
