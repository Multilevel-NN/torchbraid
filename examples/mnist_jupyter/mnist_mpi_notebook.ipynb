{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff71af3c",
   "metadata": {},
   "source": [
    "#### TODO \n",
    "\n",
    "Insert better comments, maybe a plot or two\n",
    "- grab loss from train, and then plot it...\n",
    "- give average time per batch, and min time per batch from train?\n",
    "\n",
    "Then, experiment a bit with 4 or 6 mpi tasks, 16 or 24 layers, 1 backward skip down, and 2 forward, speedup?\n",
    "- play with anything else, like num channels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98804907",
   "metadata": {},
   "source": [
    "#### Apply layer-parallel Torchbraid to simple MNIST problem (fashion or digits)\n",
    "- See `start0_install_mpi_notebook`, and `start1_simple_mpi_notebook` for setting up MPI-compatible Jupyter installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59da843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile: mpi\n",
      "IDs: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Connect to local ipython cluster.  Note, the ipcluster profile name must match the\n",
    "# below named profile. Here, we use 'mpi', but you can name the cluster profile anything\n",
    "from ipyparallel import Client, error\n",
    "cluster = Client(profile='mpi')\n",
    "print('profile:', cluster.profile)\n",
    "print(\"IDs:\", cluster.ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018636b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# You must upate the sys.path to point to your Torchbraid location\n",
    "from __future__ import print_function\n",
    "\n",
    "import statistics as stats\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from mpi4py import MPI\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"/Users/jacobschroder/joint_repos/torchbraid/torchbraid_py3_10/torchbraid\")\n",
    "sys.path.append(\"/Users/jacobschroder/joint_repos/torchbraid/torchbraid_py3_10\")\n",
    "import torchbraid\n",
    "import torchbraid.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f805aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./fashion-data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data\n",
    "# Depending on parallel setting, may want to do in parallel with `%%px` command\n",
    "from torchvision import datasets, transforms\n",
    "datasets.MNIST('./digit-data', download=True)\n",
    "datasets.FashionMNIST('./fashion-data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e775fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Network architecture is Open + ResNet + Close\n",
    "# The StepLayer defines the ResNet (ODENet) \n",
    "\n",
    "class OpenFlatLayer(nn.Module):\n",
    "  ''' \n",
    "  Opening layer has no parameters, replicates image number of channels times\n",
    "  '''\n",
    "  def __init__(self, channels):\n",
    "    super(OpenFlatLayer, self).__init__()\n",
    "    self.channels = channels\n",
    "\n",
    "  def forward(self, x):\n",
    "    s = len(x.shape) * [1]\n",
    "    s[1] = self.channels\n",
    "    x = x.repeat(s)\n",
    "    return x\n",
    "\n",
    "class CloseLayer(nn.Module):\n",
    "  '''\n",
    "  Dense closing classification layer\n",
    "  '''\n",
    "  def __init__(self, channels):\n",
    "    super(CloseLayer, self).__init__()\n",
    "    self.fc1 = nn.Linear(channels * 28 * 28, 32)\n",
    "    self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class StepLayer(nn.Module):\n",
    "  '''\n",
    "  ResNet composed of convolutional layers\n",
    "  '''\n",
    "  def __init__(self, channels):\n",
    "    super(StepLayer, self).__init__()\n",
    "    ker_width = 3\n",
    "    self.conv1 = nn.Conv2d(channels, channels, ker_width, padding=1)\n",
    "    self.conv2 = nn.Conv2d(channels, channels, ker_width, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return F.relu(self.conv2(F.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b092d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Serial network class\n",
    "class SerialNet(nn.Module):\n",
    "  def __init__(self, channels=12, local_steps=8, Tf=1.0, \n",
    "               serial_nn=None, open_nn=None, close_nn=None):\n",
    "    super(SerialNet, self).__init__()\n",
    "\n",
    "    if open_nn is None:\n",
    "      self.open_nn = OpenFlatLayer(channels)\n",
    "    else:\n",
    "      self.open_nn = open_nn\n",
    "\n",
    "    if serial_nn is None:\n",
    "      step_layer = lambda: StepLayer(channels)\n",
    "      numprocs = 1\n",
    "      parallel_nn = torchbraid.LayerParallel(MPI.COMM_SELF, step_layer, numprocs*local_steps, Tf,\n",
    "                                             max_fwd_levels=1, max_bwd_levels=1, max_iters=1)\n",
    "      parallel_nn.setPrintLevel(0, True)\n",
    "      self.serial_nn = parallel_nn.buildSequentialOnRoot()\n",
    "    else:\n",
    "      self.serial_nn = serial_nn\n",
    "\n",
    "    if close_nn is None:\n",
    "      self.close_nn = CloseLayer(channels)\n",
    "    else:\n",
    "      self.close_nn = close_nn\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.open_nn(x)\n",
    "    x = self.serial_nn(x)\n",
    "    x = self.close_nn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8852e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Parallel network class\n",
    "# local_steps: number of ResNet layers per processor\n",
    "# all other parameter definitions are in the params['...'] block\n",
    "class ParallelNet(nn.Module):\n",
    "  def __init__(self, channels=12, local_steps=8, Tf=1.0, max_levels=1, max_iters=1, \n",
    "               fwd_max_iters=0, print_level=0, braid_print_level=0, cfactor=4, \n",
    "               fine_fcf=False, skip_downcycle=True, fmg=False, relax_only_cg=0,\n",
    "               user_mpi_buf=False, gpu_direct_commu=False):\n",
    "    super(ParallelNet, self).__init__()\n",
    "\n",
    "    step_layer = lambda: StepLayer(channels)\n",
    "    numprocs = MPI.COMM_WORLD.Get_size()\n",
    "    \n",
    "    self.parallel_nn = torchbraid.LayerParallel(MPI.COMM_WORLD, step_layer, local_steps*numprocs, Tf,\n",
    "                                                max_fwd_levels=max_levels, max_bwd_levels=max_levels,\n",
    "                                                max_iters=max_iters, user_mpi_buf=user_mpi_buf,\n",
    "                                                gpu_direct_commu=gpu_direct_commu)\n",
    "    if fwd_max_iters > 0:\n",
    "      print('fwd_amx_iters', fwd_max_iters)\n",
    "      self.parallel_nn.setFwdMaxIters(fwd_max_iters)\n",
    "        \n",
    "    self.parallel_nn.setPrintLevel(print_level, True)\n",
    "    self.parallel_nn.setPrintLevel(braid_print_level, False)\n",
    "    self.parallel_nn.setCFactor(cfactor)\n",
    "    self.parallel_nn.setSkipDowncycle(skip_downcycle)\n",
    "    self.parallel_nn.setBwdRelaxOnlyCG(relax_only_cg)\n",
    "    self.parallel_nn.setFwdRelaxOnlyCG(relax_only_cg)\n",
    "    if fmg:\n",
    "      self.parallel_nn.setFMG()\n",
    "    \n",
    "    self.parallel_nn.setNumRelax(1)  # FCF relaxation default on coarse levels\n",
    "    if not fine_fcf:\n",
    "      self.parallel_nn.setNumRelax(0, level=0)  # Set F-Relaxation only on the fine grid\n",
    "    else:\n",
    "      self.parallel_nn.setNumRelax(1, level=0)  # Set FCF-Relaxation on the fine grid\n",
    "\n",
    "    # this object ensures that only the LayerParallel code runs on ranks!=0\n",
    "    compose = self.compose = self.parallel_nn.comp_op()\n",
    "\n",
    "    # by passing this through 'compose' (mean composition: e.g. OpenFlatLayer o channels)\n",
    "    # on processors not equal to 0, these will be None (there are no parameters to train there)\n",
    "    self.open_nn = compose(OpenFlatLayer, channels)\n",
    "    self.close_nn = compose(CloseLayer, channels)\n",
    "\n",
    "  def saveSerialNet(self, name):\n",
    "    serial_nn = self.parallel_nn.buildSequentialOnRoot()\n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "      s_net = SerialNet(-1, -1, -1, serial_nn=serial_nn, open_nn=self.open_nn, close_nn=self.close_nn)\n",
    "      s_net.eval()\n",
    "      torch.save(s_net, name)\n",
    "\n",
    "  def getDiagnostics(self):\n",
    "    return self.parallel_nn.getDiagnostics()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # by passing this through 'o' (mean composition: e.g. self.open_nn o x)\n",
    "    # this makes sure this is run on only processor 0\n",
    "\n",
    "    x = self.compose(self.open_nn, x)\n",
    "    x = self.parallel_nn(x)\n",
    "    x = self.compose(self.close_nn, x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b052424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Train model for one epoch\n",
    "def train(rank, params, model, train_loader, optimizer, epoch, compose, device):\n",
    "  model.train()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  total_time = 0.0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    start_time = timer()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = compose(criterion, output, target)\n",
    "    loss.backward()\n",
    "    stop_time = timer()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_time += stop_time - start_time\n",
    "    if batch_idx % params['log_interval'] == 0:\n",
    "      root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "               100. * batch_idx / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))\n",
    "\n",
    "  root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "           100. * (batch_idx + 1) / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb56f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Evaluate model on validation data\n",
    "def test(rank, model, test_loader, compose, device):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      test_loss += compose(criterion, output, target).item()\n",
    "\n",
    "      if rank == 0:\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "\n",
    "  root_print(rank, '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed73439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Parallel printing helper function\n",
    "def root_print(rank, s):\n",
    "  if rank == 0:\n",
    "    print(s)\n",
    "\n",
    "# Compute number of parallel-in-time multigrid levels\n",
    "def compute_levels(num_steps, min_coarse_size, cfactor):\n",
    "  from math import log, floor\n",
    "\n",
    "  # Find L such that ( max_L min_coarse_size*cfactor**L <= num_steps)\n",
    "  levels = floor(log(float(num_steps) / min_coarse_size, cfactor)) + 1\n",
    "\n",
    "  if levels < 1:\n",
    "    levels = 1\n",
    "  return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17afb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Set parameters for network architecture and layer-parallel \n",
    "params = {}\n",
    "params['seed'] = 1           # random seed\n",
    "params['log_interval'] = 10  # how many batches to wait before logging training status\n",
    "params['dataset'] = 'digits' # 'digits' or 'fashion' MNIST \n",
    "#\n",
    "params['steps'] = 16         # number of times steps in the resnet layer-parallel part\n",
    "params['channels'] = 4       # number of channels in resnet layer\n",
    "params['Tf'] = 1.0           # final time for resnet layer-parallel part\n",
    "#\n",
    "params['percent_data'] = 0.1 # how much of the data to read in and use for training/testing\n",
    "params['batch_size'] = 50    # input batch size for training\n",
    "params['epochs'] = 2         # number of epochs to train\n",
    "params['lr'] = 0.01          # learning rate\n",
    "#\n",
    "params['lp_max_levels'] = 3         # layer parallel max number levels \n",
    "params['lp_max_iters'] = 2          # layer parallel max iterations\n",
    "params['lp_fwd_max_iters'] = -1     # layer parallel max (forward) iterations, if -1 use lp_max_iters\n",
    "params['lp_print_level'] = 0        # layer parallel internal print level: 0, 1, 2, 3 \n",
    "params['lp_braid_print_level'] = 0  # layer parallel braid print level: 0, 1, 2, 3 \n",
    "params['lp_cfactor'] = 4            # layer parallel coarsening factor\n",
    "params['lp_fine_fcf'] = False       # layer parallel fine FCF on or off \n",
    "params['no_cuda'] = False           # disables CUDA training\n",
    "params['warm_up'] = False           # warm up for GPU timings\n",
    "params['lp_user_mpi_buf'] = False   # layer parallel use user-defined mpi buffers \n",
    "params['lp_use_downcycle']= False   # layer parallel use downcycle on or off\n",
    "params['lp_gpu_direct_commu'] = False # layer parallel GPU direct communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a3e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] No GPUs to be used, CPU only\n",
       "Run info rank: 0: | Device: cpu | Host: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Run info rank: 1: | Device: cpu | Host: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# Begin setting up run-time environment \n",
    "# MPI information\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "procs = comm.Get_size()\n",
    "\n",
    "# Use device or CPU?\n",
    "use_cuda = not params['no_cuda'] and torch.cuda.is_available()\n",
    "device, host = torchbraid.utils.getDevice(comm=comm)\n",
    "if not use_cuda:\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Run info rank: {rank}: | Device: {device} | Host: {host}')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(params['seed'])\n",
    "\n",
    "# Compute max number of layer-parallel levels\n",
    "if params['lp_max_levels'] == -1:\n",
    "  min_coarse_size = 3\n",
    "  params['lp_max_levels'] = compute_levels(params['steps'], min_coarse_size, params['lp_cfactor'])\n",
    "\n",
    "# Compute number of steps in ResNet per processor\n",
    "local_steps = int(params['steps'] / procs)\n",
    "if params['steps'] % procs != 0:\n",
    "  root_print(rank, 'Steps must be an even multiple of the number of processors: %d %d' % (params['steps'], procs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36b91cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] -- Using Digit MNIST\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# read in Digits MNIST or Fashion MNIST\n",
    "if params['dataset'] == 'digits':\n",
    "  root_print(rank, '-- Using Digit MNIST')\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                  ])\n",
    "  dataset = datasets.MNIST('./digit-data', download=False, transform=transform)\n",
    "else:\n",
    "  root_print(rank, '-- Using Fashion MNIST')\n",
    "  transform = transforms.Compose([transforms.ToTensor()])\n",
    "  dataset = datasets.FashionMNIST('./fashion-data', download=False, transform=transform)\n",
    "\n",
    "train_size = int(50000 * params['percent_data'])\n",
    "test_size = int(10000 * params['percent_data'])\n",
    "train_set = torch.utils.data.Subset(dataset, range(train_size))\n",
    "test_set = torch.utils.data.Subset(dataset, range(train_size, train_size + test_size))\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=params['batch_size'], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=params['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffcc5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Create layer-parallel network\n",
    "# Note this can be done on only one processor, but will be slow\n",
    "model = ParallelNet(channels=params['channels'],\n",
    "                    local_steps=local_steps,\n",
    "                    max_levels=params['lp_max_levels'],\n",
    "                    max_iters=params['lp_max_iters'],\n",
    "                    fwd_max_iters=params['lp_fwd_max_iters'],\n",
    "                    print_level=params['lp_print_level'],\n",
    "                    braid_print_level=params['lp_braid_print_level'],\n",
    "                    cfactor=params['lp_cfactor'],\n",
    "                    fine_fcf=params['lp_fine_fcf'],\n",
    "                    skip_downcycle=not params['lp_use_downcycle'],\n",
    "                    fmg=False, \n",
    "                    Tf=params['Tf'],\n",
    "                    relax_only_cg=False,\n",
    "                    user_mpi_buf=params['lp_user_mpi_buf'],\n",
    "                    gpu_direct_commu=params['lp_gpu_direct_commu']).to(device)\n",
    "\n",
    "compose = model.compose\n",
    "model.parallel_nn.fwd_app.setTimerFile(\n",
    "  'b_fwd_s_%d_c_%d_bs_%d_p_%d_gpuc_%d'%(params['steps'], params['channels'], params['batch_size'], procs, params['lp_gpu_direct_commu']) )\n",
    "model.parallel_nn.bwd_app.setTimerFile( \n",
    "  'b_bwd_s_%d_c_%d_bs_%d_p_%d_gpuc_%d'%(params['steps'], params['channels'], params['batch_size'], procs, params['lp_gpu_direct_commu']) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17abf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Declare optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9)\n",
    "\n",
    "# For better timings (especially with GPUs) do a little warm up\n",
    "if params['warm_up']:\n",
    "  warm_up_timer = timer()\n",
    "  train(rank=rank, args=params, model=model, train_loader=train_loader, optimizer=optimizer, epoch=0,\n",
    "        compose=compose, device=device)\n",
    "  if force_lp:\n",
    "    model.parallel_nn.timer_manager.resetTimers()\n",
    "    model.parallel_nn.fwd_app.resetBraidTimer()\n",
    "    model.parallel_nn.bwd_app.resetBraidTimer()\n",
    "  if use_cuda:\n",
    "    torch.cuda.synchronize()\n",
    "  root_print(rank, f'Warm up timer {timer() - warm_up_timer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba553705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.308671\tTime Per Batch 0.395477\n",
       "Train Epoch: 1 [500/5000 (10%)]\tLoss: 1.484470\tTime Per Batch 0.393349\n",
       "Train Epoch: 1 [1000/5000 (20%)]\tLoss: 0.762372\tTime Per Batch 0.387455\n",
       "Train Epoch: 1 [1500/5000 (30%)]\tLoss: 0.475898\tTime Per Batch 0.388980\n",
       "Train Epoch: 1 [2000/5000 (40%)]\tLoss: 0.657345\tTime Per Batch 0.387190\n",
       "Train Epoch: 1 [2500/5000 (50%)]\tLoss: 0.107705\tTime Per Batch 0.385901\n",
       "Train Epoch: 1 [3000/5000 (60%)]\tLoss: 0.462533\tTime Per Batch 0.384882\n",
       "Train Epoch: 1 [3500/5000 (70%)]\tLoss: 0.388873\tTime Per Batch 0.383329\n",
       "Train Epoch: 1 [4000/5000 (80%)]\tLoss: 0.242518\tTime Per Batch 0.382303\n",
       "Train Epoch: 1 [4500/5000 (90%)]\tLoss: 0.355126\tTime Per Batch 0.382884\n",
       "Train Epoch: 1 [5000/5000 (100%)]\tLoss: 0.518656\tTime Per Batch 0.382135\n",
       "\n",
       "Test set: Average loss: 0.0089, Accuracy: 864/1000 (86%)\n",
       "\n",
       "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.321612\tTime Per Batch 0.373599\n",
       "Train Epoch: 2 [500/5000 (10%)]\tLoss: 0.353730\tTime Per Batch 0.374180\n",
       "Train Epoch: 2 [1000/5000 (20%)]\tLoss: 0.484818\tTime Per Batch 0.377198\n",
       "Train Epoch: 2 [1500/5000 (30%)]\tLoss: 0.292148\tTime Per Batch 0.377820\n",
       "Train Epoch: 2 [2000/5000 (40%)]\tLoss: 0.311107\tTime Per Batch 0.376327\n",
       "Train Epoch: 2 [2500/5000 (50%)]\tLoss: 0.091092\tTime Per Batch 0.377008\n",
       "Train Epoch: 2 [3000/5000 (60%)]\tLoss: 0.284371\tTime Per Batch 0.377555\n",
       "Train Epoch: 2 [3500/5000 (70%)]\tLoss: 0.340982\tTime Per Batch 0.378714\n",
       "Train Epoch: 2 [4000/5000 (80%)]\tLoss: 0.264116\tTime Per Batch 0.378164\n",
       "Train Epoch: 2 [4500/5000 (90%)]\tLoss: 0.290594\tTime Per Batch 0.378554\n",
       "Train Epoch: 2 [5000/5000 (100%)]\tLoss: 0.386949\tTime Per Batch 0.378198\n",
       "\n",
       "Test set: Average loss: 0.0081, Accuracy: 870/1000 (87%)\n",
       "\n",
       "TIME PER EPOCH: 38.25 (1 std dev 38.25\n",
       "TIME PER TEST:  1.06 (1 std dev 1.06\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e88722f26a4da0a9ee0ac5684fad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# Carry out parallel training\n",
    "epoch_times = []\n",
    "test_times = []\n",
    "\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "  start_time = timer()\n",
    "  train(rank=rank, params=params, model=model, train_loader=train_loader, optimizer=optimizer, epoch=epoch,\n",
    "        compose=compose, device=device)\n",
    "  epoch_times += [timer() - start_time]\n",
    "\n",
    "  start_time = timer()\n",
    "  test(rank=rank, model=model, test_loader=test_loader, compose=compose, device=device)\n",
    "  test_times += [timer() - start_time]\n",
    "\n",
    "# Print out Braid internal timings, if desired\n",
    "#timer_str = model.parallel_nn.getTimersString()\n",
    "#root_print(rank, timer_str)\n",
    "\n",
    "root_print(rank,\n",
    "           f'TIME PER EPOCH: {\"{:.2f}\".format(stats.mean(epoch_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(epoch_times))) if len(epoch_times) > 1 else \"\"}')\n",
    "root_print(rank,\n",
    "           f'TIME PER TEST:  {\"{:.2f}\".format(stats.mean(test_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(test_times))) if len(test_times) > 1 else \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d1125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.301360\tTime Per Batch 0.086586\n",
       "Train Epoch: 1 [500/5000 (10%)]\tLoss: 1.357578\tTime Per Batch 0.089633\n",
       "Train Epoch: 1 [1000/5000 (20%)]\tLoss: 0.752395\tTime Per Batch 0.089501\n",
       "Train Epoch: 1 [1500/5000 (30%)]\tLoss: 0.560271\tTime Per Batch 0.088574\n",
       "Train Epoch: 1 [2000/5000 (40%)]\tLoss: 0.592479\tTime Per Batch 0.087377\n",
       "Train Epoch: 1 [2500/5000 (50%)]\tLoss: 0.144857\tTime Per Batch 0.087100\n",
       "Train Epoch: 1 [3000/5000 (60%)]\tLoss: 0.467642\tTime Per Batch 0.086324\n",
       "Train Epoch: 1 [3500/5000 (70%)]\tLoss: 0.498877\tTime Per Batch 0.085941\n",
       "Train Epoch: 1 [4000/5000 (80%)]\tLoss: 0.268307\tTime Per Batch 0.085771\n",
       "Train Epoch: 1 [4500/5000 (90%)]\tLoss: 0.349492\tTime Per Batch 0.086924\n",
       "Train Epoch: 1 [5000/5000 (100%)]\tLoss: 0.563418\tTime Per Batch 0.087425\n",
       "\n",
       "Test set: Average loss: 0.0092, Accuracy: 858/1000 (86%)\n",
       "\n",
       "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.346644\tTime Per Batch 0.087519\n",
       "Train Epoch: 2 [500/5000 (10%)]\tLoss: 0.452453\tTime Per Batch 0.082889\n",
       "Train Epoch: 2 [1000/5000 (20%)]\tLoss: 0.420082\tTime Per Batch 0.082775\n",
       "Train Epoch: 2 [1500/5000 (30%)]\tLoss: 0.345964\tTime Per Batch 0.082490\n",
       "Train Epoch: 2 [2000/5000 (40%)]\tLoss: 0.296049\tTime Per Batch 0.082598\n",
       "Train Epoch: 2 [2500/5000 (50%)]\tLoss: 0.087568\tTime Per Batch 0.083531\n",
       "Train Epoch: 2 [3000/5000 (60%)]\tLoss: 0.250928\tTime Per Batch 0.085038\n",
       "Train Epoch: 2 [3500/5000 (70%)]\tLoss: 0.397861\tTime Per Batch 0.085992\n",
       "Train Epoch: 2 [4000/5000 (80%)]\tLoss: 0.201679\tTime Per Batch 0.085464\n",
       "Train Epoch: 2 [4500/5000 (90%)]\tLoss: 0.280768\tTime Per Batch 0.086291\n",
       "Train Epoch: 2 [5000/5000 (100%)]\tLoss: 0.461665\tTime Per Batch 0.085851\n",
       "\n",
       "Test set: Average loss: 0.0071, Accuracy: 894/1000 (89%)\n",
       "\n",
       "TIME PER EPOCH: 8.95 (1 std dev 8.95\n",
       "TIME PER TEST:  0.28 (1 std dev 0.28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49147c73e2f041e2b1ccdf01d928cbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# Create Serial network, if desired, for comparison\n",
    "serial_model = SerialNet(channels=params['channels'], local_steps=local_steps, \n",
    "                         Tf=params['Tf']).to(device)\n",
    "serial_compose = lambda op, *p: op(*p)\n",
    "\n",
    "# Declare optimizer\n",
    "optimizer = optim.SGD(serial_model.parameters(), lr=params['lr'], momentum=0.9)\n",
    "epoch_times = []\n",
    "test_times = []\n",
    "\n",
    "# Begin training\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "  start_time = timer()\n",
    "  train(rank=rank, params=params, model=serial_model, train_loader=train_loader, \n",
    "        optimizer=optimizer, epoch=epoch,compose=serial_compose, device=device)\n",
    "  epoch_times += [timer() - start_time]\n",
    "\n",
    "  start_time = timer()\n",
    "  test(rank=rank, model=serial_model, test_loader=test_loader, \n",
    "       compose=serial_compose, device=device)\n",
    "  test_times += [timer() - start_time]\n",
    "\n",
    "root_print(rank,\n",
    "           f'TIME PER EPOCH: {\"{:.2f}\".format(stats.mean(epoch_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(epoch_times))) if len(epoch_times) > 1 else \"\"}')\n",
    "root_print(rank,\n",
    "           f'TIME PER TEST:  {\"{:.2f}\".format(stats.mean(test_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(test_times))) if len(test_times) > 1 else \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7725b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
