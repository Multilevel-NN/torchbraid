{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff71af3c",
   "metadata": {},
   "source": [
    "Insert better comments, maybe a plot or two\n",
    "\n",
    "Move this file, and two simple files\n",
    "- Put two MPI simple notebooks there, maybe rename as start0_install_mpi_notebook, start1_simple_mpi_notebook,  start2_mnist_mpi_notebook\n",
    "\n",
    "Create block where you create and time serial network\n",
    "\n",
    "Then, experiment a bit with 4 or 6 mpi tasks, 16 or 24 layers, 1 backward skip down, and 2 forward, speedup?\n",
    "- play with anything else, like num channels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98804907",
   "metadata": {},
   "source": [
    "### Apply layer-parallel Torchbraid to simple MNIST problem (fashion or digits)\n",
    "- See `start0_install_mpi_jupyter`, and `start1_simple_mpi_notebook` for setting up MPI-compatible Jupyter installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59da843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/ipyparallel/util.py:210: RuntimeWarning: IPython could not determine IPs for sleepy.scott.math.unm.edu.1.0.10.in-addr.arpa: [Errno 8] nodename nor servname provided, or not known\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Connect to local ipython cluster.  Note, the ipcluster profile name must match with the below text. \n",
    "# Here, we use 'mpi', but you can name the cluster profile anything\n",
    "from ipyparallel import Client, error\n",
    "cluster = Client(profile='mpi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018636b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# You must upate the sys.path to point to your Torchbraid location\n",
    "from __future__ import print_function\n",
    "\n",
    "import statistics as stats\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from mpi4py import MPI\n",
    "\n",
    "import sys; sys.path.append(\"/Users/jacobschroder/joint_repos/torchbraid/torchbraid_py3_10/torchbraid\")\n",
    "import torchbraid\n",
    "import torchbraid.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f805aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./fashion-data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the data\n",
    "# Depending on parallel setting, may want to do in parallel with `%%px` command\n",
    "from torchvision import datasets, transforms\n",
    "datasets.MNIST('./digit-data', download=True)\n",
    "datasets.FashionMNIST('./fashion-data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e775fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Open, Close, and Hidden (Step) Layer architectures\n",
    "\n",
    "class OpenFlatLayer(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super(OpenFlatLayer, self).__init__()\n",
    "    self.channels = channels\n",
    "\n",
    "  def forward(self, x):\n",
    "    # this bit of python magic simply replicates each image in the batch\n",
    "    s = len(x.shape) * [1]\n",
    "    s[1] = self.channels\n",
    "    x = x.repeat(s)\n",
    "    return x\n",
    "\n",
    "\n",
    "class CloseLayer(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super(CloseLayer, self).__init__()\n",
    "    self.fc1 = nn.Linear(channels * 28 * 28, 32)\n",
    "    self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.fc1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class StepLayer(nn.Module):\n",
    "  def __init__(self, channels):\n",
    "    super(StepLayer, self).__init__()\n",
    "    ker_width = 3\n",
    "    self.conv1 = nn.Conv2d(channels, channels, ker_width, padding=1)\n",
    "    self.conv2 = nn.Conv2d(channels, channels, ker_width, padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return F.relu(self.conv2(F.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b092d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Serial network class (only used for comparison to parallel network)\n",
    "class SerialNet(nn.Module):\n",
    "  def __init__(self, channels=12, local_steps=8, Tf=1.0, serial_nn=None, open_nn=None, close_nn=None):\n",
    "    super(SerialNet, self).__init__()\n",
    "\n",
    "    if open_nn is None:\n",
    "      self.open_nn = OpenFlatLayer(channels)\n",
    "    else:\n",
    "      self.open_nn = open_nn\n",
    "\n",
    "    if serial_nn is None:\n",
    "      step_layer = lambda: StepLayer(channels)\n",
    "      numprocs = 1\n",
    "      parallel_nn = torchbraid.LayerParallel(MPI.COMM_SELF, step_layer, numprocs * local_steps, Tf,\n",
    "                                             max_fwd_levels=1, max_bwd_levels=1, max_iters=1)\n",
    "      parallel_nn.setPrintLevel(0, True)\n",
    "      self.serial_nn = parallel_nn.buildSequentialOnRoot()\n",
    "    else:\n",
    "      self.serial_nn = serial_nn\n",
    "\n",
    "    if close_nn is None:\n",
    "      self.close_nn = CloseLayer(channels)\n",
    "    else:\n",
    "      self.close_nn = close_nn\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.open_nn(x)\n",
    "    x = self.serial_nn(x)\n",
    "    x = self.close_nn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8852e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "# Parallel network class\n",
    "class ParallelNet(nn.Module):\n",
    "  def __init__(self, channels=12, local_steps=8, Tf=1.0, max_levels=1, max_iters=1, fwd_max_iters=0, print_level=0,\n",
    "               braid_print_level=0, cfactor=4, fine_fcf=False, skip_downcycle=True, fmg=False, relax_only_cg=0,\n",
    "               user_mpi_buf=False, gpu_direct_commu=False):\n",
    "    super(ParallelNet, self).__init__()\n",
    "\n",
    "    step_layer = lambda: StepLayer(channels)\n",
    "\n",
    "    numprocs = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "    self.parallel_nn = torchbraid.LayerParallel(MPI.COMM_WORLD, step_layer, local_steps * numprocs, Tf,\n",
    "                                                max_fwd_levels=max_levels, max_bwd_levels=max_levels,\n",
    "                                                max_iters=max_iters, user_mpi_buf=user_mpi_buf,\n",
    "                                                gpu_direct_commu=gpu_direct_commu)\n",
    "    if fwd_max_iters > 0:\n",
    "      print('fwd_amx_iters', fwd_max_iters)\n",
    "      self.parallel_nn.setFwdMaxIters(fwd_max_iters)\n",
    "    self.parallel_nn.setPrintLevel(print_level, True)\n",
    "    self.parallel_nn.setPrintLevel(braid_print_level, False)\n",
    "    self.parallel_nn.setCFactor(cfactor)\n",
    "    self.parallel_nn.setSkipDowncycle(skip_downcycle)\n",
    "    self.parallel_nn.setBwdRelaxOnlyCG(relax_only_cg)\n",
    "    self.parallel_nn.setFwdRelaxOnlyCG(relax_only_cg)\n",
    "\n",
    "    if fmg:\n",
    "      self.parallel_nn.setFMG()\n",
    "    self.parallel_nn.setNumRelax(1)  # FCF elsewehre\n",
    "    if not fine_fcf:\n",
    "      self.parallel_nn.setNumRelax(0, level=0)  # F-Relaxation on the fine grid\n",
    "    else:\n",
    "      self.parallel_nn.setNumRelax(1, level=0)  # F-Relaxation on the fine grid\n",
    "\n",
    "    # this object ensures that only the LayerParallel code runs on ranks!=0\n",
    "    compose = self.compose = self.parallel_nn.comp_op()\n",
    "\n",
    "    # by passing this through 'compose' (mean composition: e.g. OpenFlatLayer o channels)\n",
    "    # on processors not equal to 0, these will be None (there are no parameters to train there)\n",
    "    self.open_nn = compose(OpenFlatLayer, channels)\n",
    "    self.close_nn = compose(CloseLayer, channels)\n",
    "\n",
    "  def saveSerialNet(self, name):\n",
    "    serial_nn = self.parallel_nn.buildSequentialOnRoot()\n",
    "    if MPI.COMM_WORLD.Get_rank() == 0:\n",
    "      s_net = SerialNet(-1, -1, -1, serial_nn=serial_nn, open_nn=self.open_nn, close_nn=self.close_nn)\n",
    "      s_net.eval()\n",
    "      torch.save(s_net, name)\n",
    "\n",
    "  def getDiagnostics(self):\n",
    "    return self.parallel_nn.getDiagnostics()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # by passing this through 'o' (mean composition: e.g. self.open_nn o x)\n",
    "    # this makes sure this is run on only processor 0\n",
    "\n",
    "    x = self.compose(self.open_nn, x)\n",
    "    x = self.parallel_nn(x)\n",
    "    x = self.compose(self.close_nn, x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfb56f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Training function for one epoch    \n",
    "def train(rank, params, model, train_loader, optimizer, epoch, compose, device):\n",
    "  model.train()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  total_time = 0.0\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    start_time = timer()\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = compose(criterion, output, target)\n",
    "    loss.backward()\n",
    "    stop_time = timer()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_time += stop_time - start_time\n",
    "    if batch_idx % params['log_interval'] == 0:\n",
    "      root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "               100. * batch_idx / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))\n",
    "\n",
    "  root_print(rank, 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime Per Batch {:.6f}'.format(\n",
    "    epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "           100. * (batch_idx + 1) / len(train_loader), loss.item(), total_time / (batch_idx + 1.0)))\n",
    "\n",
    "# Evaluate model on validation data\n",
    "def test(rank, model, test_loader, compose, device):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      data, target = data.to(device), target.to(device)\n",
    "      output = model(data)\n",
    "      test_loss += compose(criterion, output, target).item()\n",
    "\n",
    "      if rank == 0:\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "\n",
    "  root_print(rank, '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed73439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Parallel printing helper function\n",
    "def root_print(rank, s):\n",
    "  if rank == 0:\n",
    "    print(s)\n",
    "\n",
    "# Compute number of parallel in time levels\n",
    "def compute_levels(num_steps, min_coarse_size, cfactor):\n",
    "  from math import log, floor\n",
    "\n",
    "  # we want to find $L$ such that ( max_L min_coarse_size*cfactor**L <= num_steps)\n",
    "  levels = floor(log(float(num_steps) / min_coarse_size, cfactor)) + 1\n",
    "\n",
    "  if levels < 1:\n",
    "    levels = 1\n",
    "  return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17afb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "#Set default parameters for network and layer-parallel \n",
    "params = {}\n",
    "params['seed'] = 1           # random seed\n",
    "params['log_interval'] = 10  # how many batches to wait before logging training status\n",
    "params['dataset'] = 'digits' # 'digits' or 'fashion' MNIST \n",
    "params['serial_file'] = None # load starting network from file\n",
    "#\n",
    "params['steps'] = 16         # number of times steps in the resnet layer-parallel part\n",
    "params['channels'] = 4       # number of channels in resnet layer\n",
    "params['tf'] = 1.0           # final time for resnet layer-parallel part\n",
    "#\n",
    "params['percent_data'] = 1.0 # how much of the data to read in and use for training/testing\n",
    "params['batch_size'] = 50    # input batch size for training\n",
    "params['epochs'] = 2         # number of epochs to train\n",
    "params['lr'] = 0.01          # learning rate\n",
    "#\n",
    "params['force_lp'] = False   # use layer parallel even if there is only 1 MPI rank\n",
    "params['lp_levels'] = 3      # max number layer parallel levels \n",
    "params['lp_iters'] = 2       # layer parallel iterations\n",
    "params['lp_fwd_iters'] = -1  # layer parallel (forward) iterations, if -1 use lp-iters\n",
    "params['lp_print'] = 0       # layer parallel internal print level: 0, 1, 2, 3 \n",
    "params['lp_braid_print'] = 0 # layer parallel braid print level: 0, 1, 2, 3 \n",
    "params['lp_cfactor'] = 4     # layer parallel coarsening factor\n",
    "params['lp_finefcf'] = False # layer parallel fine FCF on or off \n",
    "params['no_cuda'] = False    # disables CUDA training\n",
    "params['warm_up'] = False    # warm up for GPU timings\n",
    "params['lp_gpu_direct_commu'] = False # layer parallel GPU direct communication\n",
    "params['lp_user_mpi_buf'] = False     # layer parallel use user-defined mpi buffers \n",
    "params['lp_use_downcycle']= False     # layer parallel use downcycle on or off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81a3e5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] No GPUs to be used, CPU only\n",
       "Run info rank: 0: Torch version: 1.12.1 | Device: cpu | Host: cpu\n",
       "MNIST ODENet:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Run info rank: 1: Torch version: 1.12.1 | Device: cpu | Host: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# something\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "procs = comm.Get_size()\n",
    "    \n",
    "use_cuda = not params['no_cuda'] and torch.cuda.is_available()\n",
    "\n",
    "device, host = torchbraid.utils.getDevice(comm=comm)\n",
    "if not use_cuda:\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f'Run info rank: {rank}: Torch version: {torch.__version__} | Device: {device} | Host: {host}')\n",
    "\n",
    "# some logic to default to Serial if on one processor,\n",
    "# can be overriden by the user to run layer-parallel\n",
    "if params['force_lp']:\n",
    "  force_lp = True\n",
    "elif procs > 1:\n",
    "  force_lp = True\n",
    "else:\n",
    "  force_lp = False\n",
    "\n",
    "torch.manual_seed(params['seed'])\n",
    "\n",
    "if params['lp_levels'] == -1:\n",
    "  min_coarse_size = 3\n",
    "  params['lp_levels'] = compute_levels(params['steps'], min_coarse_size, params['lp_cfactor'])\n",
    "\n",
    "local_steps = int(params['steps'] / procs)\n",
    "if params['steps'] % procs != 0:\n",
    "  root_print(rank, 'Steps must be an even multiple of the number of processors: %d %d' % (params['steps'], procs))\n",
    "  sys.exit(0)\n",
    "\n",
    "root_print(rank, 'MNIST ODENet:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36b91cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] -- Using Digit MNIST\n",
       "-- procs    = 2\n",
       "-- channels = 4\n",
       "-- tf       = 1.0\n",
       "-- steps    = 16\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "# read in Digits MNIST or Fashion MNIST\n",
    "if params['dataset']:\n",
    "  root_print(rank, '-- Using Digit MNIST')\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                  ])\n",
    "  dataset = datasets.MNIST('./digit-data', download=False, transform=transform)\n",
    "else:\n",
    "  root_print(rank, '-- Using Fashion MNIST')\n",
    "  transform = transforms.Compose([transforms.ToTensor()])\n",
    "  dataset = datasets.FashionMNIST('./fashion-data', download=False, transform=transform)\n",
    "# if params['digits']\n",
    "\n",
    "root_print(rank, '-- procs    = {}\\n'\n",
    "                 '-- channels = {}\\n'\n",
    "                 '-- tf       = {}\\n'\n",
    "                 '-- steps    = {}'.format(procs, params['channels'], params['tf'], params['steps']))\n",
    "\n",
    "train_size = int(50000 * params['percent_data'])\n",
    "test_size = int(10000 * params['percent_data'])\n",
    "train_set = torch.utils.data.Subset(dataset, range(train_size))\n",
    "test_set = torch.utils.data.Subset(dataset, range(train_size, train_size + test_size))\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=params['batch_size'], shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "root_print(rank, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2623b6fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] Using ParallelNet:\n",
       "-- max_levels     = 3\n",
       "-- max_iters      = 2\n",
       "-- fwd_iters      = -1\n",
       "-- cfactor        = 4\n",
       "-- fine fcf       = False\n",
       "-- skip down      = True\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "if force_lp:\n",
    "  root_print(rank, 'Using ParallelNet:')\n",
    "  root_print(rank, '-- max_levels     = {}\\n'\n",
    "                   '-- max_iters      = {}\\n'\n",
    "                   '-- fwd_iters      = {}\\n'\n",
    "                   '-- cfactor        = {}\\n'\n",
    "                   '-- fine fcf       = {}\\n'\n",
    "                   '-- skip down      = {}\\n'.format(params['lp_levels'],\n",
    "                                                     params['lp_iters'],\n",
    "                                                     params['lp_fwd_iters'],\n",
    "                                                     params['lp_cfactor'],\n",
    "                                                     params['lp_finefcf'],\n",
    "                                                     not params['lp_use_downcycle'] ))\n",
    "  model = ParallelNet(channels=params['channels'],\n",
    "                      local_steps=local_steps,\n",
    "                      max_levels=params['lp_levels'],\n",
    "                      max_iters=params['lp_iters'],\n",
    "                      fwd_max_iters=params['lp_fwd_iters'],\n",
    "                      print_level=params['lp_print'],\n",
    "                      braid_print_level=params['lp_braid_print'],\n",
    "                      cfactor=params['lp_cfactor'],\n",
    "                      fine_fcf=params['lp_finefcf'],\n",
    "                      skip_downcycle=not params['lp_use_downcycle'],\n",
    "                      fmg=False, \n",
    "                      Tf=params['tf'],\n",
    "                      relax_only_cg=False,\n",
    "                      user_mpi_buf=params['lp_user_mpi_buf'],\n",
    "                      gpu_direct_commu=params['lp_gpu_direct_commu']).to(device)\n",
    "\n",
    "  if params['serial_file'] is not None:\n",
    "    model.saveSerialNet(params['serial_file'])\n",
    "  compose = model.compose\n",
    "\n",
    "  model.parallel_nn.fwd_app.setTimerFile(\n",
    "    'b_fwd_s_%d_c_%d_bs_%d_p_%d_gpuc_%d'%(params['steps'], params['channels'], params['batch_size'], procs, params['lp_gpu_direct_commu']) )\n",
    "  model.parallel_nn.bwd_app.setTimerFile( \n",
    "    'b_bwd_s_%d_c_%d_bs_%d_p_%d_gpuc_%d'%(params['steps'], params['channels'], params['batch_size'], procs, params['lp_gpu_direct_commu']) )   \n",
    "else:\n",
    "  root_print(rank, 'Using SerialNet:')\n",
    "  root_print(rank, '-- serial file = {}\\n'.format(params['serial_file']))\n",
    "  if params['serial_file'] is not None:\n",
    "    print('loading model')\n",
    "    model = torch.load(params['serial_file'])\n",
    "  else:\n",
    "    model = SerialNet(channels=params['channels'], local_steps=local_steps, Tf=params['tf']).to(device)\n",
    "  compose = lambda op, *p: op(*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17abf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "optimizer = optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9)\n",
    "\n",
    "epoch_times = []\n",
    "test_times = []\n",
    "\n",
    "# check out the initial conditions\n",
    "# if force_lp:\n",
    "#   diagnose(rank, model, test_loader,0)\n",
    "\n",
    "if params['warm_up']:\n",
    "  warm_up_timer = timer()\n",
    "  train(rank=rank, args=params, model=model, train_loader=train_loader, optimizer=optimizer, epoch=0,\n",
    "        compose=compose, device=device)\n",
    "  if force_lp:\n",
    "    model.parallel_nn.timer_manager.resetTimers()\n",
    "    model.parallel_nn.fwd_app.resetBraidTimer()\n",
    "    model.parallel_nn.bwd_app.resetBraidTimer()\n",
    "  if use_cuda:\n",
    "    torch.cuda.synchronize()\n",
    "  epoch_times = []\n",
    "  test_times = []\n",
    "  root_print(rank, f'Warm up timer {timer() - warm_up_timer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85d1125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.285251\tTime Per Batch 0.385654\n",
       "Train Epoch: 1 [500/50000 (1%)]\tLoss: 1.149751\tTime Per Batch 0.384923\n",
       "Train Epoch: 1 [1000/50000 (2%)]\tLoss: 0.869054\tTime Per Batch 0.384235\n",
       "Train Epoch: 1 [1500/50000 (3%)]\tLoss: 0.599639\tTime Per Batch 0.382953\n",
       "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 0.694879\tTime Per Batch 0.383972\n",
       "Train Epoch: 1 [2500/50000 (5%)]\tLoss: 0.236841\tTime Per Batch 0.383715\n",
       "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 0.381839\tTime Per Batch 0.382314\n",
       "Train Epoch: 1 [3500/50000 (7%)]\tLoss: 0.492649\tTime Per Batch 0.385873\n",
       "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 0.272742\tTime Per Batch 0.390947\n",
       "Train Epoch: 1 [4500/50000 (9%)]\tLoss: 0.287633\tTime Per Batch 0.395515\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560526e963214e30961945830c11a8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/2 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received Keyboard Interrupt. Sending signal SIGINT to engines...\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "  start_time = timer()\n",
    "  train(rank=rank, params=params, model=model, train_loader=train_loader, optimizer=optimizer, epoch=epoch,\n",
    "        compose=compose, device=device)\n",
    "  end_time = timer()\n",
    "  epoch_times += [end_time - start_time]\n",
    "\n",
    "  start_time = timer()\n",
    "\n",
    "  test(rank=rank, model=model, test_loader=test_loader, compose=compose, device=device)\n",
    "  end_time = timer()\n",
    "  test_times += [end_time - start_time]\n",
    "\n",
    "  # print out some diagnostics\n",
    "  # if force_lp:\n",
    "  #  diagnose(rank, model, test_loader,epoch)\n",
    "\n",
    "if force_lp:\n",
    "  timer_str = model.parallel_nn.getTimersString()\n",
    "  root_print(rank, timer_str)\n",
    "\n",
    "root_print(rank,\n",
    "           f'TIME PER EPOCH: {\"{:.2f}\".format(stats.mean(epoch_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(epoch_times))) if len(epoch_times) > 1 else \"\"}')\n",
    "root_print(rank,\n",
    "           f'TIME PER TEST:  {\"{:.2f}\".format(stats.mean(test_times))} '\n",
    "           f'{(\"(1 std dev \" + \"{:.2f}\".format(stats.mean(test_times))) if len(test_times) > 1 else \"\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559444e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Move all diagnose stuff here...\n",
    "\n",
    "def diagnose(rank, model, test_loader, epoch):\n",
    "  model.parallel_nn.diagnostics(True)\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  itr = iter(test_loader)\n",
    "  data, target = next(itr)\n",
    "\n",
    "  # compute the model and print out the diagnostic information\n",
    "  with torch.no_grad():\n",
    "    output = model(data)\n",
    "\n",
    "  diagnostic = model.getDiagnostics()\n",
    "\n",
    "  if rank != 0:\n",
    "    return\n",
    "\n",
    "  features = np.array([diagnostic['step_in'][0]] + diagnostic['step_out'])\n",
    "  params = np.array(diagnostic['params'])\n",
    "\n",
    "  fig, axs = pyplot.subplots(2, 1)\n",
    "  axs[0].plot(range(len(features)), features)\n",
    "  axs[0].set_ylabel('Feature Norm')\n",
    "\n",
    "  coords = [0.5 + i for i in range(len(features) - 1)]\n",
    "  axs[1].set_xlim([0, len(features) - 1])\n",
    "  axs[1].plot(coords, params, '*')\n",
    "  axs[1].set_ylabel('Parameter Norms: {}/tstep'.format(params.shape[1]))\n",
    "  axs[1].set_xlabel('Time Step')\n",
    "\n",
    "  fig.suptitle('Values in Epoch {}'.format(epoch))\n",
    "\n",
    "  # pyplot.show()\n",
    "  pyplot.savefig('diagnose{:03d}.png'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
